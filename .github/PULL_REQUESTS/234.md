# WIP: Example infra using one account and one region

**conorgil** commented *Jun 15, 2017*

**WIP! Opened for discussion purposes. This is not ready to merge.**

There is still a lot of work and cleanup to be done on this example, but I think it is in a state that people can run it and provide some feedback. I want to get some feedback from the community to see if I am on the right track for creating something that people will find useful.

Is the level of detail in this example too much? Too little? Confusing? Helpful? Any and all feedback is appreciated so I can take it into account as I work on this and other examples.

*Before merging eventually, the Terraform modules can be recreated in repos owned by Gruntworks instead of me.*
<br />
***


**conorgil** commented *Jun 15, 2017*

@brikis98 In this example, I recommend maintaining each Terraform module in its own repo. In #169, it looks like you recommend using a single repo called `modules` that has a subdir for each Terraform module. Thoughts on the pros/cons of each approach?

Benefits to having each module in its own repo:
* It is easier to reason about version numbers throughout the infrastructure. If each module has its own version, then you can easily identify which environments are using different versions of a given module. If all modules are defined in a single repo, then you could have prod using `v0.1.1` and staging using `v0.1.2` and you're not actually sure whether `modules/A` changed between those two versions. It could have easily been a change applied to `modules/B` that caused the version number for the repo to jump. For me, this reason alone is enough motivation to host each in its own repo.
* Improved maintainability and usability
  * the README.md for each module can reside at the root of the repo and can clearly document what the module does, the variables, etc
  * the CHANGELOG.md for each module can reside at the root of the repo and clearly document important changes over time, etc
  * Logical place to add automated tests to make sure that the module works as expected

Drawbacks to having each in its own module:
* there are more repos to manage, more admin overhead
* there can be dependencies between Terraform modules and maintaining them in separate repos could make it more difficult to keep them in sync
***

**mcalhoun** commented *Jun 15, 2017*

> In this example, I recommend maintaining each Terraform module in its own repo

@conorgil I also agree with this approach. The one additional benefit it brings occurs during the development of complex packages. The automated test cycle runs on a CI platform can become very long when each module's tests need to complete with each new commit. Having a module isolated could significantly lower the "wait for tests to run" time.
***

**dpetzel** commented *Jun 15, 2017*

This is really great (wishing I had it when I first got started). I'm wondering about having different structures for one account/1 region and multiple account/multiple region, versus just laying out a structure that works for both right from the beginning? Jumping to multiple account and multiple region was a challenge for me for sure.
***

**conorgil** commented *Jun 15, 2017*

@mcalhoun Great point about CI/CD running tests more quickly.

@dpetzel thanks for all of your comments! I'll address each inline. 

Regarding your point about accounts and regions, I was thinking about this same thing today. I think that it would make sense for someone just starting out with Terragrunt to actually set up their directory structure such that they could easily add additional regions or accounts in the future even if they only start with a single one in the beginning. I have not personally had to set those situations up yet, but I'll be playing with them over the next few weeks so that I can write a good tutorial on them. Any pointers or tips you can share about your experience would be super useful!
***

**brikis98** commented *Jun 16, 2017*

@conorgil This is great work, thank you!

Some thoughts:

1. It would be great to add automated tests to these examples. However, deploying real AWS resources (e.g. EC2 Instances, RDS, etc) takes a while, and has to be carefully cleaned up. Perhaps we keep the folder-structure as-is, but change the contents of each example to `template_file` data sources or other Terraform elements that are purely local and very quick to set up. The comments in the code would explain why we're doing this, and the various modules can still depend on each other (e.g. you could create an output variable with the rendered text of one of the `template_file` values in one module and pull in that rendered text in another module using `terraform_remote_state`), but we'd also be able to add fast-running automated tests on top of these examples to ensure they stay up to date over time. The `integration_test.go` folder has similar examples.

1. Keeping your modules in separate repos is completely fine. To get versioning, they should be defined in *at least* one separate repo (e.g. `live` and `modules`), but having multiple separate repo has plenty of advantages, as you and @mcalhoun mentioned.

1. It would be great to have an example that's a bit simpler as a starting point. It would have several environments (stage, prod), but each environment would contain a single `terraform.tfvars` file. This works well for small deployments, as there are fewer files, less code to manage, no need to use `terraform_remote_state`, easier testing, and so on. Of course, it has the downside of making it possible to break the entire environment with any mistake, but in some use cases, that's an acceptable trade-off, and we should call that out explicitly in the docs. 
***

**conorgil** commented *Jun 16, 2017*

@brikis98 

1. Great idea to add automated tests to the examples. I have not written automated tests for Terraform code before. Do you have any guidance for the types of tests that we could write? Would you just write the tests in Go, or are you aware of any tools to help write IAC or Terraform specific tests? I looked at [test/integration_test.go](https://github.com/gruntwork-io/terragrunt/blob/master/test/integration_test.go) for some ideas. We could write a simple test to execute `terragrunt apply-all` and `terragrunt destroy-all` and make sure that each executes without error. Thoughts?

2. Agreed.

3. I agree that a simpler example might be a better starting point, but I think that might be too simple. Few questions to make sure I'm on the same page, though. Would each env have a single `main.tf` file and a `terraform.tfvars` file, or would the env only have a `terraform.tfvars` file and use remote source? If using remote source, what would the remote source look like? If using local source, then what do you imagine the configuration in `terraform.tfvars` containing?

    I think the first tutorial really needs to highlight the value of Terragrunt and which pain points it solves to make it clear why someone would want to use it. Given, we've all had different experiences with Terraform, so the content of that tutorial might vary on preference and experience.

I have been working on a presentation that I am going to give to a Terraform meetup group in Berlin in early July. My approach is to walk through a series of steps to take a Terraform project from "very disorganized" to "best practice" as I understand it to be. Each step will make a single change to the project and I will discuss the motivation for the change. As things progress, certain pain points will emerge that someone might think "that is going to a PITA with Terraform even though I see the value we're hoping to get with this change". That is when I will introduce Terragrunt and show how it can solve those pain points so that we can have our cake and eat it too.

Thoughts on that general approach for explaining the benefit of Terragrunt? We could modle the examples in this repo in a similar fashion if I get positive feedback from the meetup group and folks in this thread.

***

**brikis98** commented *Jun 16, 2017*

>  I looked at test/integration_test.go for some ideas. We could write a simple test to execute terragrunt apply-all and terragrunt destroy-all and make sure that each executes without error. Thoughts?

Yes, exactly. 

> Would each env have a single main.tf file and a terraform.tfvars file, or would the env only have a terraform.tfvars file and use remote source? If using remote source, what would the remote source look like? If using local source, then what do you imagine the configuration in terraform.tfvars containing?

In the `live` repo, you'd just have a single `terraform.tfvars` file per env. In the `modules` repo, you'd have a single `main.tf` (or a single folder of Terraform files) defining the entire environment. It's a fairly common set up for users who don't want to split up components into separate modules and use `terraform_remote_state` to connect them. 

That said, it's definitely not a blocker for this PR, but merely a suggestion for other examples we could add in the future.

> My approach is to walk through a series of steps to take a Terraform project from "very disorganized" to "best practice" as I understand it to be. 

That sounds like a great talk!

> Thoughts on that general approach for explaining the benefit of Terragrunt?

I think your approach sounds like a great way to do it. Show the pain, show the solution, and discuss trade-offs. Please share the link with us if that talk is available online in the future :)
***

**conorgil** commented *Jun 20, 2017*

@brikis98 
> In the live repo, you'd just have a single terraform.tfvars file per env. In the modules repo, you'd have a single main.tf (or a single folder of Terraform files) defining the entire environment. It's a fairly common set up for users who don't want to split up components into separate modules and use terraform_remote_state to connect them.

I have been thinking a lot about this scenario recently and I am confident that I am thinking myself in circles and could use some outside perspective. Can you help me grok why someone would want to use Terragrunt in this scenario instead of plain Terraform?

With Terragrunt, it would look like:
```
live
├─ terraform.tfvars         # Defines the configuration patterns for the S3 backend
├─production
│   └─ terraform.tfvars     # Has source="git::modules.git?ref=v0.1.1"
│                           # Defines the input var values for production.
│                           # Includes partial config of S3 backend.
└─staging
       └─ terraform.tfvars  # Has source="git::modules.git?ref=v0.2.0"
                            # Defines the input var values for staging.
                            # Includes partial config of S3 backend.
```

With Terraform, it would look like (**note:** this example is broken from copy/paste error, see following comments in thread):
```
live
├─production
│   └─ terraform.tfvars     # Has source="git::modules.git?ref=v0.1.1"
│                           # Defines the input var values for production.
│                           # Defines and configures the S3 backend for production.
└─staging
       └─ terraform.tfvars  # Has source="git::modules.git?ref=v0.2.0"
                            # Defines the input var values for staging.
                            # Defines and configures the S3 backend for staging.
```

Both approaches have the same user workflow (a single `terraform/grunt plan` and `terraform/grunt apply` per env), both use versioned modules, and both store a single state file in S3 per env.

In this use-case, is the main motivation for using Terragrunt instead of Terraform to remain DRY and abstract the configuration of the S3 backend configuration in each env? I can see this providing more and more value each time an env is added, but seems like a small win with only two envs. Are there any other benefits of Terragrunt in this use-case that I'm over looking?

Generally, it seems to me that Terragrunt is *way* more valuable when working with a state file per infrastructure component than state file per env. However, I am still forming a concise argument for why that structure is a good idea in the first place. I'd love input from others as I continue to think on it!
***

**brikis98** commented *Jun 21, 2017*

> With Terraform, it would look like:

The purely Terraform example you list isn't supported by Terraform natively, at all. There is no support for `source = ...` in `.tfvars` files, so you'd have to have, at the bare minimum, a `.tf` file that calls out the module you want, sets the `source` URL in that module, and sets all the parameters within it. In practice, you also need to define a `provider`, and expose input variables, and add output variables, so instead of a single `.tfvars` or single `.tf` file, you have several, with a non-trivial amount of code. And all of that has to be copy/pasted into each environment.
***

**conorgil** commented *Jun 22, 2017*

@brikis98 Yup, the Terraform example in my previous comment is totally broken. Thanks for pointing that out. I copy/pasted and guess I didn't really read over things carefully after changing the comments in the example.

I'm still unclear why one would choose to use Terragrunt over Terraform in this simple scenario (two envs, single state file for each). In both cases, you have almost the same amount of code, just in different places.

In the Terraform setup, the local code is heavy and the remote module is lean. The main.tf file sets up the provider, the backend, and calls the remote module using the `module { source = ... }` syntax; the variables.tf file defines the input variables; and the output.tf file defines the output values. The remote module defines the resources, the input variables, and the output values.

In the Terragrunt setup, the local code is slightly lighter and the remote module is slightly heavier. The `terrform.tfvars` file now is responsible for calling the remote module using Terragrunt `source = ...` syntax and defining the input variable values. The remote module would define the provider (exposing the config as variables), define the backend partial, define input variables, and define output values. 

Yes, there is some copy/paste between environments in the Terraform approach, but I think that is a small price to pay compared to the overhead of introducing another tool into the development stack for such a simple use-case (single state file per env). I see some value using Terragrunt to remain as DRY as possible if the number of environments gets much larger, but in a small example like this it seems like overkill to me. Of course, Terragrunt would obviously work in this scenario and if someone thinks the benefits of being completely DRY outweigh the tradeoff of adding a new tool to the stack, then power to em! I'm just trying to understand the motivations for the use-case and wanted to see if I was missing some part of the bigger picture.
***

**brikis98** commented *Jun 22, 2017*

> I'm still unclear why one would choose to use Terragrunt over Terraform in this simple scenario (two envs, single state file for each). In both cases, you have almost the same amount of code, just in different places.

It's not the same amount of code. Here's an example.

Say you have some Terraform code in `infra-modules/my-infra/main.tf` that defines a way to deploy your entire infrastructure, including auto scaling groups, load balancers, security groups, IAM roles, etc:

```hcl
resource "aws_autoscaling_group" "example" {
  # ...
}

resource "aws_launch_configuration" "example" {
  # ...
}

resource "aws_alb" "example" {
  # ...
}
```

This module takes in a bunch of input variables in `infra-modules/my-infra/vars.tf`:

```hcl
variable "aws_region" {
  description = "The AWS region to deploy into (e.g. us-east-1)"
}

variable "aws_account_id" {
  description = "The ID of the AWS account to deploy into"
}

variable "min_size" {
  description = "The minimum number of instances of the microservice to run in the ASG"
}

variable "max_size" {
  description = "The maximum number of instances of the microservice to run in the ASG"
}

variable "instance_type" {
  description = "The type of instance to run in the ASG"
}

# ...
```

It may also have a bunch of output variables in `infra-modules/my-infra/outputs.tf`:

```hcl
output "alb_dns_name" {
  value = "${aws_alb.example.dns_name}"
}

output "asg_name" {
  value = "${aws_autoscaling_group.example.name}"
}

output "iam_role_id" {
  value = "${aws_iam_role.example.id}"
}

# ...
```

Now, to use this module in infra-live with pure Terraform, you would typically need the following code in `infra-live/stage/main.tf`:

```hcl
module "stage" {
  source = "../infra-live/my-infra"
  
  aws_region = "us-east-1"
  aws_account_id = "1234567890123"

  min_size = 3
  max_size = 5
  instance_type = "t2.micro"

  # ...
}
```

To keep your code DRY and to allow some of those params to be set dynamically (e.g. via env vars, `.tfvars` files, cmd-line params, etc), you'll need to expose some of those params as input variables in `infra-live/stage/vars.tf`:

```hcl
variable "aws_region" {
  description = "The AWS region to deploy into (e.g. us-east-1)"
}

variable "aws_account_id" {
  description = "The ID of the AWS account to deploy into"
}

# ...
```

And you have to mirror just about all of the output variables from the module in `infra-live/stage/outputs.tf` so they are displayed to users and stored in Terraform state:

```hcl
output "alb_dns_name" {
  value = "${module.stage.alb_dns_name}"
}

output "asg_name" {
  value = "${module.stage.asg_name}"
}

output "iam_role_id" {
  value = "${module.stage.iam_role_id}"
}

# ...
```

Now you have to copy/paste all three files into each environment: `infra-live/prod`, `infra-live/qa`, etc. 

Compare that with the Terragrunt approach, where the code in `infra-modules` is more or less the same, but the code in infra-live is reduced to a single `infra-live/stage/terraform.tfvars` file:

```hcl
terragrunt = {
  terraform {
    source = "../infra-live/my-infra"
  }
}

aws_region = "us-east-1"
aws_account_id = "1234567890123"

min_size = 3
max_size = 5
instance_type = "t2.micro"
```

The key idea is there is no `vars.tf` or `outputs.tf` to copy/paste and maintain infra-live. With a tiny bit of code and 1-2 environments, it's not a huge issue. But as the amount of infrastructure grows--as it usually does--and as you add more input and output variables, and as the number of environment grows, it becomes more and more painful. Many of our customers have large, multi-account deployments, with many different environments, and Terragrunt has proven useful for keeping their Terraform code maintainable.
***

**conorgil** commented *Jun 23, 2017*

@brikis98 Thanks for the detailed example. There is definitely more duplication that I thought. Consider me convinced!
***

**brikis98** commented *Jul 2, 2017*

@conorgil and all other Terragrunt users: I've created a couple example repos for Terragrunt users:

https://github.com/gruntwork-io/terragrunt-infrastructure-modules-example
https://github.com/gruntwork-io/terragrunt-infrastructure-live-example

Take a look and let me know if this provides helpful examples on how to set things up. PRs are very welcome! In the meantime, I'll add links to these repos to the Terragrunt docs.
***

**brikis98** commented *Jan 7, 2020*

PR not updated in several years, closing.
***

