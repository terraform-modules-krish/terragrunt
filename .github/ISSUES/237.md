# `plan-all` complaining root terraform config is a module and has no configuration files

**davidvuong** commented *Jun 18, 2017*

Project structure:

```
.
├── README.md
├── packer
│   ├── Dockerfile
│   └── ubuntu-16.04
│       ├── scripts
│       │   └── bootstrap.sh
│       └── template.json
└── terraform
    ├── Dockerfile
    ├── terraform.tfvars
    ├── tf_common.tfvars
    ├── tf_secrets.tfvars
    └── tf_vpc
        ├── main.tf
        ├── terraform.tfvars
        └── variables.tf

5 directories, 11 files
```

When I run `terragrunt plan-all` inside `./terraform`, I get this:

```
[terragrunt] [/deployments/terraform] 2017/06/18 04:35:25 Running command: /bin/terraform --version
[terragrunt] 2017/06/18 04:35:26 Stack at /deployments/terraform:
  => Module /deployments/terraform/tf_vpc (dependencies: [])
  => Module /deployments/terraform (dependencies: [])
[terragrunt] [/deployments/terraform/tf_vpc] 2017/06/18 04:35:26 Module /deployments/terraform/tf_vpc must wait for 0 dependencies to finish
[terragrunt] [/deployments/terraform/tf_vpc] 2017/06/18 04:35:26 Running module /deployments/terraform/tf_vpc now
[terragrunt] [/deployments/terraform/tf_vpc] 2017/06/18 04:35:26 Reading Terragrunt config file at /deployments/terraform/tf_vpc/terraform.tfvars
[terragrunt] [/deployments/terraform] 2017/06/18 04:35:26 Module /deployments/terraform must wait for 0 dependencies to finish
[terragrunt] [/deployments/terraform] 2017/06/18 04:35:26 Running module /deployments/terraform now
[terragrunt] [/deployments/terraform] 2017/06/18 04:35:26 Reading Terragrunt config file at /deployments/terraform/terraform.tfvars
[terragrunt] [/deployments/terraform] 2017/06/18 04:35:26 Initializing remote state for the s3 backend
[terragrunt] [/deployments/terraform/tf_vpc] 2017/06/18 04:35:26 Initializing remote state for the s3 backend
[terragrunt] [/deployments/terraform] 2017/06/18 04:35:28 Configuring remote state for the s3 backend
[terragrunt] [/deployments/terraform] 2017/06/18 04:35:28 Running command: /bin/terraform init -backend-config=bucket=xxx -backend-config=key=./terraform.tfstate -backend-config=region=us-east-1 -backend-config=encrypt=true -backend-config=lock_table=terraform-lock-table
[terragrunt] [/deployments/terraform/tf_vpc] 2017/06/18 04:35:28 Configuring remote state for the s3 backend
[terragrunt] [/deployments/terraform/tf_vpc] 2017/06/18 04:35:28 Running command: /bin/terraform init -backend-config=region=us-east-1 -backend-config=encrypt=true -backend-config=lock_table=terraform-lock-table -backend-config=bucket=xxx -backend-config=key=tf_vpc/terraform.tfstate
[terragrunt] [/deployments/terraform] 2017/06/18 04:35:28 Running command: /bin/terraform plan -var-file=/deployments/terraform/tf_secrets.tfvars -var-file=/deployments/terraform/tf_common.tfvars
[terragrunt] [/deployments/terraform] 2017/06/18 04:35:29 Module /deployments/terraform has finished with an error: exit status 1
[terragrunt] [/deployments/terraform/tf_vpc] 2017/06/18 04:35:33 Running command: /bin/terraform plan -var-file=/deployments/terraform/tf_secrets.tfvars -var-file=/deployments/terraform/tf_common.tfvars
Acquiring state lock. This may take a few moments...
Refreshing Terraform state in-memory prior to plan...
The refreshed state will be used to calculate this plan, but will not be
persisted to local or remote state storage.

The Terraform execution plan has been generated and is shown below.
Resources are shown in alphabetical order for quick scanning. Green resources
will be created (or destroyed and then created if an existing resource
exists), yellow resources are being changed in-place, and red resources
will be destroyed. Cyan entries are data sources to be read.

Note: You didn't specify an "-out" parameter to save this plan, so when
"apply" is called, Terraform can't guarantee this is what will execute.
...
...
```

It seems `terragrunt` thinks the root terraform directory is a module. `terraform` proceeds to run `plan` but can't find any configuration files and returns with a non-zero exit code.

```
  => Module /deployments/terraform (dependencies: [])
...
[terragrunt] [/deployments/terraform] 2017/06/18 04:35:29 Module /deployments/terraform has finished with an error: exit status 1
```

Is there a way to tell `terragrunt` to not consider the root of my terraform config as a module? I've tried specifying `--terragrunt-working-dir=$(pwd)` but it doesn't help:

```
TF_LOG=1 terragrunt plan-all --terragrunt-working-dir=$(pwd) --terragrunt-non-interactive

[terragrunt] [/deployments/terraform] 2017/06/18 04:49:22 Running command: /bin/terraform --version
[terragrunt] 2017/06/18 04:49:23 Stack at /deployments/terraform:
  => Module /deployments/terraform (dependencies: [])
  => Module /deployments/terraform/tf_vpc (dependencies: [])
[terragrunt] [/deployments/terraform] 2017/06/18 04:49:23 Module /deployments/terraform must wait for 0 dependencies to finish
[terragrunt] [/deployments/terraform/tf_vpc] 2017/06/18 04:49:23 Module /deployments/terraform/tf_vpc must wait for 0 dependencies to finish
[terragrunt] [/deployments/terraform/tf_vpc] 2017/06/18 04:49:23 Running module /deployments/terraform/tf_vpc now
[terragrunt] [/deployments/terraform/tf_vpc] 2017/06/18 04:49:23 Reading Terragrunt config file at /deployments/terraform/tf_vpc/terraform.tfvars
[terragrunt] [/deployments/terraform] 2017/06/18 04:49:23 Running module /deployments/terraform now
[terragrunt] [/deployments/terraform] 2017/06/18 04:49:23 Reading Terragrunt config file at /deployments/terraform/terraform.tfvars
[terragrunt] [/deployments/terraform] 2017/06/18 04:49:23 Initializing remote state for the s3 backend
[terragrunt] [/deployments/terraform/tf_vpc] 2017/06/18 04:49:23 Remote state is already configured for backend s3
[terragrunt] [/deployments/terraform/tf_vpc] 2017/06/18 04:49:23 Running command: /bin/terraform plan -var-file=/deployments/terraform/tf_secrets.tfvars -var-file=/deployments/terraform/tf_common.tfvars --terragrunt-working-dir=/deployments/terraform
[terragrunt] [/deployments/terraform/tf_vpc] 2017/06/18 04:49:23 Module /deployments/terraform/tf_vpc has finished with an error: exit status 1
[terragrunt] [/deployments/terraform] 2017/06/18 04:49:26 Configuring remote state for the s3 backend
[terragrunt] [/deployments/terraform] 2017/06/18 04:49:26 Running command: /bin/terraform init -backend-config=region=us-east-1 -backend-config=encrypt=true -backend-config=lock_table=terraform-lock-table -backend-config=bucket=xxx -backend-config=key=./terraform.tfstate
[terragrunt] [/deployments/terraform] 2017/06/18 04:49:26 Running command: /bin/terraform plan -var-file=/deployments/terraform/tf_secrets.tfvars -var-file=/deployments/terraform/tf_common.tfvars --terragrunt-working-dir=/deployments/terraform
[terragrunt] [/deployments/terraform] 2017/06/18 04:49:26 Module /deployments/terraform has finished with an error: exit status 1
[terragrunt] 2017/06/18 04:49:26 Encountered the following errors:
exit status 1
exit status 1
```

The plan on `tf_vpc` still goes through and it seems to be working but the non-zero error can raise concerns for others reading the deploy logs.
<br />
***


**brikis98** commented *Jun 18, 2017*

It's running `plan` in your root director because there is a `terraform.tfvars` file there. Currently, the best option is to have one more layer of directories below root, which useful anyway to define multiple environments:

```
.
├── README
├── packer
│   ├── Dockerfile
│   └── ubuntu-16.04
│       ├── scripts
│       │   └── bootstrap.sh
│       └── template.json
└── terraform
    ├── Dockerfile
    ├── prod
    │   └── tf_vpc
    │       ├── main.tf
    │       ├── terraform.tfvars
    │       └── variables.tf
    ├── stage
    │   └── tf_vpc
    │       ├── main.tf
    │       ├── terraform.tfvars
    │       └── variables.tf
    ├── terraform.tfvars
    ├── tf_common.tfvars
    └── tf_secrets.tfvars
```

If you run `plan-all` in `stage` or `prod`, it should work correctly.
***

**davidvuong** commented *Jun 18, 2017*

Thanks for that.

The project I'm working on is just a personal site and it doesn't have more than one environment right now so I might just live with the non-zero until I add another.

However, I'll keep that in mind when migrating from 0.11.x to 0.12.x in another project of mine.
***

