# How to handle long running process (in my case local Tiller) ?

**barryib** commented *Sep 9, 2019*

I would like to use terraform to bootstrap a newly created EKS cluster, by installing tools like: ALB ingress controller, AWS sport termination handler or cluster autoscaler. I want to use helm chart for that, but without installing Tiller in the cluster.

For that, I use the so called "[tiller-less](https://rimusz.net/tillerless-helm)" principle with the [helm-tiller plugin](https://github.com/rimusz/helm-tiller). I run tiller locally, and tell the helm provider to use the local tiller.

When I run my terraform scripts, I was able to run tiller in background and install charts. But at the end of terraform plan or apply, terragrunt hang and wait for tiller to stop. Any suggestion on how to handle this ?

Here are sample of my scripts.

#### I use external datasource to run tiller-less

This will run tiller in background and return the port as external data source.

```shell
#!/bin/bash
set -e

eval "$(jq -r '@sh "KUBECONFIG=\(.kubeconfig) PORT=\(.port) PROBE_PORT=\(.probe_port) NAMESPACE=\(.namespace)"')"

# Create kubeconfig
KUBECONFIG_FILE_NAME="helm-$NAMESPACE-$PORT"
echo "$KUBECONFIG" > "$KUBECONFIG_FILE_NAME"

HELM_TILLER_PORT=$PORT HELM_TILLER_PROBE_PORT=$PROBE_PORT KUBECONFIG=$(realpath $KUBECONFIG_FILE_NAME) helm tiller start-ci $NAMESPACE > /dev/null

jq -n --arg host "localhost:$PORT" '{"host":$host}'

```

#### Terraform samples

```hcl
provider "helm" {
  host           = data.external.tiller_kube_system.result.host
  namespace      = "kube-system"
  install_tiller = false
  init_helm_home = false
}

data "external" "tiller_kube_system" {
  program     = ["${path.module}/scripts/tiller.sh"]
  working_dir = path.module

  query = {
    kubeconfig = module.eks.kubeconfig
    port       = local.tiller_kube_system_port
    probe_port = local.tiller_kube_system_probe_port
    namespace  = "kube-system"
  }
}

data "helm_repository" "incubator" {
  name = "incubator"
  url  = "https://kubernetes-charts-incubator.storage.googleapis.com"
}

resource "helm_release" "alb_ingress_controller" {
  name      = "aws-alb-ingress-controller"
  namespace = "kube-system"
  wait      = true

  repository = data.helm_repository.incubator.metadata.0.name
  chart      = "aws-alb-ingress-controller"
  version    = var.alb_ingress_controler_chart_version
  
  # ....
}
```

I'm getting with this, the end message of terraform plan or apply

```
------------------------------------------------------------------------

Note: You didn't specify an "-out" parameter to save this plan, so Terraform
can't guarantee that exactly these actions will be performed if
"terraform apply" is subsequently run.
```

I tried to use after hook to handle this, but terragrunt doesn't run hooks and hang untill I kill tiller process.

#### Terragrunt sample

```hcl
terraform {
  source = "xxxx"

  after_hook "stop_tillers" {
    commands     = ["apply", "plan"]
    execute      = ["helm", "tiller", "stop"]
    run_on_error = true
  }
}
```

Any suggestion ?
<br />
***


**barryib** commented *Sep 9, 2019*

Here is how I'm solving this for now

I touch a file (`.tiller-started`) into the external datasource and trigger null_resource when the file exist.

```hcl
resource "null_resource" "close_tillers" {
  provisioner "local-exec" {
    working_dir = path.module
    command     = "helm tiller stop; rm -f ${path.module}/.tiller-started"
    on_failure  = "continue"
  }

  triggers = {
    t = fileexists("${path.module}/.tiller-started") ? timestamp() : ".tiller-started"
  }

  depends_on = [
    ALL_YOUR_HELM_RELEASES
  ]
}
```

This is ok for now. The downside of this methode, is
- plan always show changes
- I have to add all helm release as dependency of the null resource
- it doesn't work with terraform destroy

Can this be handled correctly with a terragrunt `after_hook` ?
***

**yorinasub17** commented *Sep 9, 2019*

I would suggest using before and after hooks in terragrunt to start and stop the tiller server before calling terraform. IIRC, `helm-tiller` plugin starts the server on a known port locally so you can hard code the host and port. Otherwise, I can see it being difficult managing the dependencies within terraform to properly handle the lifecycle, especially when you have to call modules.

Alternatively, you can use `helm tiller run -- terragrunt apply`, to let the plugin manage the lifecycle.
***

**barryib** commented *Sep 9, 2019*

@yorinasub17 this is not ideal, because tiller need some creds after EKS creation.

By the way, I tried to use before hooks. It doesn't work either. The before hook hang and don't run terraform.

```shell
[terragrunt] 2019/09/09 21:25:52 Executing hook: start_tiller_kube_system
[terragrunt] 2019/09/09 21:25:52 Running command: helm tiller start-ci kube-system
Installed Helm version v2.14.3
Installed Tiller version v2.14.3
Helm and Tiller are the same version!
Set the following vars to use tiller:
export TILLER_NAMESPACE=kube-system
export HELM_HOST=127.0.0.1:44134
Starting Tiller...
Tiller namespace: kube-system


```

Here are my hooks

```hcl
  before_hook "start_tiller_kube_system" {
    commands     = ["apply", "plan"]
    execute      = ["helm", "tiller", "start-ci", "kube-system"]
    run_on_error = false
  }

  after_hook "stop_tillers" {
    commands     = ["apply", "plan"]
    execute      = ["helm", "tiller", "stop"]
    run_on_error = true
  }
```

When I kill the tiller process, terragrunt continue its work and launch terraform normally.
***

**yorinasub17** commented *Sep 9, 2019*

> this is not ideal, because tiller need some creds after EKS creation.

I highly recommend separating out the modules so that you aren't trying to deploy both EKS and tiller in the same module. In my experience, chaining provider setup in terraform to resources is highly finicky and don't always work, especially in the face of destroy.

---

It sounds like there is some kind of issue with the way the tiller plugin exits or spawns the tiller process that is not compatible with terragrunt hooks. PRs are welcome if someone more familiar can dig in to investigate.
***

**barryib** commented *Sep 9, 2019*

> I highly recommend separating out the modules so that you aren't trying to deploy both EKS and tiller in the same module. In my experience, chaining provider setup in terraform to resources is highly finicky and don't always work, especially in the face of destroy.

Yep. But I'm not trying to install tiller. I'm just starting and stopping it locally. It's a part of the CI.

> It sounds like there is some kind of issue with the way the tiller plugin exits or spawns the tiller process that is not compatible with terragrunt hooks. PRs are welcome if someone more familiar can dig in to investigate.

I'll try to dig a bit.
***

**yorinasub17** commented *Sep 9, 2019*

> But I'm not trying to install tiller

Sorry, I meant the module setting up the Helm provider to talk to Tiller. I was assuming you were stringing the credentials to the helm command within terraform, based on

> tiller need some creds after EKS creation.

Basically, what I was suggesting was:

- Run module to setup EKS.
- Setup `kubectl` (or set it up inline in CI via a call to `aws eks configure`)
- Run `helm tiller run -- terragrunt apply` on the module that needs the local tiller.

Note I recognize this isn't ideal, since it doesn't quite work with `terragrunt apply-all`, so I am not suggesting this is the full solution. Just an alternative workaround.
***

**barryib** commented *Sep 10, 2019*

After some digging, it sounds like the blocking point is the [`cmd.wait()`](https://github.com/gruntwork-io/terragrunt/blob/master/shell/run_shell_cmd.go#L85) which normal.

The `cmd.wait()` will wait until all child process to complete.

Plus, from the godoc:

> If any of c.Stdin, c.Stdout or c.Stderr are not an *os.File, Wait also waits for the respective I/O loop copying to or from the process to complete.

I don't know how I can avoid this for background process (orphans). Any thoughts ?
***

**barryib** commented *Sep 11, 2019*

After all, I think that the best place to do this is in the helm provider to add tillerless support.
***

**barryib** commented *Sep 13, 2019*

@yorinasub17 Here is a first try to solve the tillerless problem https://github.com/terraform-providers/terraform-provider-helm/pull/339
***

**aav66** commented *Oct 13, 2021*

 There is a way to not lock with  `cmd.wait() `: run this cmd like this: nohup cmd &>/dev/null &
***

