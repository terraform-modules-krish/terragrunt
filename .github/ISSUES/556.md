# apply-all is failing due to lock on a terraform_remote_state object on azure

**bfleming-ciena** commented *Aug 29, 2018*

All my objects access the same terraform_remote_state object for the network layer.  When running apply-all I get this.

Error: Error refreshing state: 1 error(s) occurred:

* data.terraform_remote_state.network: 1 error(s) occurred:

* data.terraform_remote_state.network: data.terraform_remote_state.network: error loading the remote state: failed to lock azure state: 2 error(s) occurred

Is this a bug? How do I work around this?  I'm guessing that there is a brief lock on the state file before it's ready and due to terragrunt running this all concurrently it's hitting this?


<br />
***


**brikis98** commented *Aug 29, 2018*

We use lots of `terraform_remote_state` in parallel too, and I've never seen a lock error... AFAIK, `terraform_remote_state` is a `data` source and should be 100% read-only... So I have no clue why it would even be trying to assume a lock! Perhaps file a bug in the Terraform or Azure provider repos?
***

**bfleming-ciena** commented *Aug 30, 2018*

Thank you @brikis98. That is helpful to know, maybe it is something else, on my side. I'll double check.


***

**bfleming-ciena** commented *Aug 30, 2018*

@brikis98,

The error I get is about the state file that I'm using as a data source, so it is trying to lock. This is azure, so maybe that's why?  My data source is network information at:

tfstates/proj/vnet/terraform.tfstateenv:test



```data "terraform_remote_state" "network" {
  backend   = "azurerm"
  workspace = "${terraform.workspace}"

  config {
    resource_group_name  = "${local.workspace["remote_state_resource_group_name"]}"
    access_key           = "${var.access_key}"
    storage_account_name = "${local.workspace["tfstate_storage_account_name"]}"
    container_name       = "${local.workspace["container_name"]}"
    key                  = "${local.workspace["remote_state_key"]}"
  }
}

data.terraform_remote_state.network: data.terraform_remote_state.network: error loading the remote state: failed to lock azure state: storage: service returned error: StatusCode=409, ErrorCode=LeaseAlreadyPresent, ErrorMessage=There is already a lease present.
azurerm_virtual_machine.vm-windows-with-datadisk: Refreshing state... (ID: /subscriptions/136a4f50-fd70-4723-887c-...Compute/virtualMachines/AZW1WSPINDEXTT)
RequestId:aaaaa
Time:2018-08-30T03:02:21.8722544Z, RequestInitiated=Thu, 30 Aug 2018 03:02:21 GMT, RequestId=aaaaa, API Version=2016-05-31, QueryParameterName=, QueryParameterValue=
Lock Info:
  ID:        abc
  Path:      tfstates/proj/vnet/terraform.tfstateenv:test
  Operation: init
  Who:      me
  Version:   0.11.8
  Created:   2018-08-30 03:02:21.716017052 +0000 UTC
  Info:
```
***

**bfleming-ciena** commented *Aug 30, 2018*

If I just acquire the lease on the state file from I am using as a data source, and run terraform apply, I get the error, so terraform itself locks the file while reading it.

This means, the concurrent apply-all is broken for azure. I blame TF, not tgrunt.

But is there some way I can tell terragrunt to just do apply-all sequentially?  Being able to apply-all is the primary reason I'm trying to use terragrunt, otherwise I just write a script. :)


***

**brikis98** commented *Aug 30, 2018*

Wow, that is weird behavior by Terraform indeed!

The ability to set parallelism to 1 on `apply-all`, `destroy-all`, etc would actually be useful in a variety of use cases, including yours. Since Terragrunt uses goroutines for concurrency, I think we'd just need to set [GOMAXPROCS](https://golang.org/pkg/runtime/#GOMAXPROCS) to 1. We could expose a new `--terragrunt-concurrency` parameter and set `GOMAXPROCS` accordingly.

Would anyone be up for a quick PR to do that?
***

**bfleming-ciena** commented *Aug 30, 2018*

@brikis98 Really appreciate the quick responses, thank you. I had hopes that I could use the -lock-timeout=10s option with apply-all, but that didn't work. It really seemed to make sense that would work just fine, if it would just re-try the lock or wait a bit. Ah well, the concurrency option would be great.
***

**matbest1** commented *Aug 11, 2020*

Just landed here in this two years old discussion, I'm also facing this problem.. If I keep the state local everything works fine.. @bfleming-ciena have you been able to find a good solution to this problem?

***

**yorinasub17** commented *Aug 12, 2020*

While I don't have a good solution for the underlying issue, the proposed workaround to limit concurrency is now available with [terragrunt-parallelism](https://terragrunt.gruntwork.io/docs/reference/cli-options/#terragrunt-parallelism).
***

**yorinasub17** commented *Sep 27, 2021*

Closing as won't fix.

This is inherently a problem in the way state is managed in Azure and I don't see a way to work around it in terragrunt other than the above mentioned `terragrunt-parallelism` CLI option.

One new option (for folks who land here) is to switch to using `dependency` blocks (https://terragrunt.gruntwork.io/docs/features/execute-terraform-commands-on-multiple-modules-at-once/#passing-outputs-between-modules), although I suspect the same locking issues will exist.
***

