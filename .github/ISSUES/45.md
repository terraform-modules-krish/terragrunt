# Add the ability to automatically walk the dependency tree of modules

**brikis98** commented *Nov 1, 2016*

When following [Terraform state management best practices](https://blog.gruntwork.io/how-to-manage-terraform-state-28f5697e68fa), you often end up with the all the components of a single environment broken into many folders. For example, here is a typical production environment:

```
prod
  └ vpc
  └ services
      └ frontend-app
      └ backend-app
          └ vars.tf
          └ outputs.tf
          └ main.tf
          └ .terragrunt
  └ data-storage
      └ mysql
      └ redis
```

The reason we break out these components into separate folders rather than defining them all in one set of templates provides is to ensure *isolation*. That way, when you're making changes to one component (e.g. deploying the frontend-app), you aren't risking breaking all the others (e.g. breaking the VPC, data storage, etc). 

The question is, how do you "spin up" an entire environment without having to manually go into each folder and run `terragrunt apply`? 

One option is to add a command like `terragrunt spin-up` which could find all the Terraform modules in the folder structure (based on the presence of a `.tf` files), parse the code in those modules to figure out the dependencies between them (that is, those dependencies explicitly specified via the usage of the [terraform_remote_state resource](https://www.terraform.io/docs/providers/terraform/d/remote_state.html)), and automatically call `terragrunt apply` on that dependency tree in the right order. There could be an analogous `terragrunt spin-down` command too. If all the modules contain `.terragrunt` files (or there is a root `.terragrunt` file if we implement https://github.com/gruntwork-io/terragrunt/issues/26#issuecomment-260665542), then Terragrunt would know how they manage remote state, and could therefore match the remote state configuration against the usage of the `terraform_remote_state` resource within each module.

This would work as long as all dependencies between modules are specified via the `terraform_remote_state` resource. If there are an implicit dependencies—for example, if the `iam` module creates an IAM role and the `kms` module has the name of that IAM role copy/pasted and just assumes it'll exist—then you may get errors. The good news is that fixing those errors would be very simply: just add a `terraform_remote_state` resource to the `kms` module that points to the remote state of the `iam` module. In other words, adding a `terraform_remote_state` resource would be like adding a `depends_on` parameter.

There is also the tricky question of what to do if Terragrunt hits an error part of the way through spinning up an environment. Probably the right thing to do is to exit, just as Terraform does. If you fix the issue and re-run `terragrunt spin-up`, you'd probably lose some time running `terraform apply` on modules that have already been applied, but since there won't be any actual changes to apply, that shouldn't take too long. 
<br />
***


**brikis98** commented *Nov 15, 2016*

I just had an interesting realization, and while it's not fully formed, I wanted to jot it down here.

One of the problems with splitting up your Terraform code into many separate subfolders is not only that it's hard to run `terragrunt apply` a bunch of times, but also that you have to copy & paste all of those folders to create a new environment. For example, consider the following folder structure for _one_ environment:

```
env
  └ vpc
  └ services
      └ backend-service
          └ vars.tf
          └ outputs.tf
          └ main.tf
          └ .terragrunt
      └ frontend-service
      └ search-service
      └ profile-service
  └ data-storage
      └ mysql
      └ redis
```

Imagine you wanted to have 10 environments: e.g. one QA env, one staging env, and 8 production envs, each in a different AWS region (us-east-1, us-west-2, ap-southeast-1, etc). You would have to copy/paste all the files and folders above 10 times, update variables all over the place, and then try to maintain all of that forever. That's a bit painful.

If we had a `terragrunt spin-up` command, and it accepted a `--var-file` parameter (which defaults to `terraform.tfvars`) that it passes on to the `terraform apply` commands it's running under the hood, then we have an interesting new possibility. You could define the file structure above just once, with none of the variables filled in, and then define each of your environments as a `.terragrunt` and `.tfvars` file:

```
env
  └ vpc
  └ services
  └ data-storage
  └ environments
      └ qa
          └ terraform.tfvars
          └ .terragrunt            
      └ staging.tfvars
      └ us-east-1.tfvars
      └ us-west-2.tfvars
      └ ap-southeast-1.tfvars
```

The `.terragrunt` file would be configured as explained in https://github.com/gruntwork-io/terragrunt/issues/26#issuecomment-260665542. The `.tfvars` files would contain the unique settings for that environment. For example, `qa/terraform.tfvars` might have:

``` hcl
services {
  search-service {
    instance_type = "t2.micro"
    cluster_size = 2
  }
}

data-stores {
  mysql {
    allocated_storage = 10
    read_replicas = 0
  }
}
```

Whereas `us-east-1/terraform.tfvars` might have:

``` hcl
services {
  search-service {
    instance_type = "m4.large"
    cluster_size = 20  
  }
}

data-stores {
  mysql {
    allocated_storage = 1000
    read_replicas = 2
  }
}
```

To deploy the QA environment, you would run:

```
> cd env/environments/qa
> terragrunt spin-up
```

When you run this command, Terragrunt would walk through all of the components (vpc, data-stores, services) in the proper order, and for each component, call `terraform apply` on it with the subsection of `qa/terraform.tfvars` file that belongs to that component (e.g. when in `services/search-service` it would look up `services.search-service` in `qa/terraform.tfvars` and pass along its contents). As explained in https://github.com/gruntwork-io/terragrunt/issues/26#issuecomment-260665542, Terragrunt would also be able to figure out the proper place to store the remote state for each environment thanks to interpolation in the `.terragrunt` file.

If you made some changes to the the `qa/terraform.tfvars` file or the Terraform templates for a single service, to deploy them to the QA environment, you would run:

```
> cd env/environments/qa
> terragrunt apply --templates-path ../../services/frontend-service
```

To create an entirely new environment, you would just make a copy of an existing one (e.g. `environments/us-east-1`) and update the variables accordingly. This reduces copy/paste to two files (instead of dozens of folders and files) and each of those files contains _just_ the differences between different environments, which is something you want to update anyway (i.e. it's not overhead).

In short, this approach lets you approximate the convenience of having all your Terraform code in one set of templates, while still keeping the state for each component and environment completely separate.

There are some flaws with the approach above:
- It changes your work flow. Normally, if you want to see what's deployed, you just browse the code in a folder. If you made a change to that code, you just run `terraform apply`. With the scheme above, to see what's deployed, you have to look both in the folder, and at a `.tfvars` file. To apply a change, you have to go into one folder (e.g. the folder for one environment) and pass in the other folder's location (e.g. the folder with the backend-service templates) as parameters. 
- If the Terraform templates are all in one folder, then any change to those templates would immediately be picked up by all environments on the next deployment. This happens with all Terraform code, and the typical solution is to put the templates in a separate repo and use a Git URL with different versions in different environments (see [How to create reusable infrastructure with Terraform modules](https://blog.gruntwork.io/how-to-create-reusable-infrastructure-with-terraform-modules-25526d65f73d)). However, with the approach above, that doesn't work, because Terraform does not allow you to use variables in the `source` parameter of a version. Therefore, you couldn't define different version numbers in the `.tfvars` files.

_Update_: one alternative is to put a `<environment-name>.tfvars` file in each of the component folders. For example, `vpc/qa.tfvars` and `vpc/us-east-1.tfvars`; `services/backend-service/qa.tfvars` and `services/backend-service/us-east-1.tfvars`; and so on. That way, Terragrunt doesn't have to figure out which settings belong to which components. The downside is that you have lots of `.tfvars` files scattered all over your folders. 

***

**josh-padnick** commented *Nov 15, 2016*

@brikis98 It actually sounds like we're trying to solve two separate problems here:
1. How do we spin up an entire environment in one command?
2. How do we create the files for a new environment without having to manually change lots of variables?

I maintain these are related, but separate, problems.

**Spin Up in One Command**

On the first problem, I'm a fan of `terragrunt spin-up` being run from an environment's root folder. As you suggest, it could recurisvely scan all `.terragrunt` files in the child directories, build a dependency graph, and then `terragrunt apply` these one at a time. 

It seems like we would have to require that all dependencies be reflected as `data.terraform_remote_state` entries. Hard-coded VPC ID's, for example, wouldn't allow us to know that a VPC should be created first, or where to get that VPC ID from. 

Right now, I don't see any other issues beyond what you mentioned in your original comment (e.g. how to handle errors).

**Quickly Create the Files for a New Environment**

There are some really interesting ideas here, but my primary concern is that we are "complecting" (in the [Rich Hickey](https://www.infoq.com/presentations/Simple-Made-Easy) sense) the use of Terragrunt. In the approaches above, I have to now remember:
- Use Terraform (simple)
- Use Terragrunt (simple)
- An environment is made up of (1) the core TF files and (2) my env-specific `terraform.tfvars` file (two concepts intertwined)
- Making a change is complex, as you pointed out in your second comment (I'm now intertwining core TF files, their specific version, my env-specific `terraform.tfvars` file, and all the permutations of whether my new core TF file changes are compatible with all existing env-specific vars; it feels hard).
  ​
  As a side note on this, you can always reference your own repo via git URLs to get versions, but I don't like the idea of a repo referencing other versions of itself.

For quickly creating new environments, I believe there are some fundamental constraints we have to maintain:
1. code == environment (your first point)
2. changes can be done in isolation, then pushed to an environment of your choice
3. guardrails at every step to help you avoid a mistake

Does it make sense to explore code generation as an alternative? Maybe we use terragrunt (or boilerplate) to generate the code for an environment. When you have environment changes, you edit only the core files, never the generated files. Then you just generate the latest, and use existing git tooling to see the diff and submit a PR. We'd probably also need a way to know from which version of the original templates a particular set of code was generated from.

This way, I can generate an environment on the spot from the latest code. I can also tear it down with `terragrunt spin-down` and deleting the files. I can maintain different versions of different environments, for example by generating my stage environment with the newest patterns, testing it, and then generating prod.

And at every step, my code reflects what is actually live.

To be honest, I'm not sure if I consider the code-generation piece within the scope of boilerplate, so my vote goes to punting on the "create new environment files" problem for now and implementing a `terragrunt spin-up` and `terragrunt spin-down` command.

***

**brikis98** commented *Nov 16, 2016*

> I maintain these are related, but separate, problems.

Agreed. We should solve them separately and incrementally, but I just wanted to get down my thoughts in writing somewhere :)

> Making a change is complex [...]

The reality is that when you break things up by component and especially when you use modules, making changes is already complex. The "make all changes in one template" flow really only exists for the simplest Terraform examples.

> Does it make sense to explore code generation as an alternative

What's the point of generating code if you can't edit that code?

> And at every step, my code reflects what is actually live.

This is true of the proposal I made above. The code captures 100% of what's live. To do that, you don't need to copy/paste the code 20 times. Having "per environment" files that reference the code accomplishes the same goal without the copy/paste. 

***

**josh-padnick** commented *Nov 17, 2016*

Separately, we agreed to defer the file-creation discussion, but just to record my thoughts for future reference...

> What's the point of generating code if you can't edit that code?

So that the code reflects what's deployed. As we've previously discussed, code generation has an initial generation step but then gets out of sync with the latest generator templates. If we were to constantly re-generate the code, we avoid that issue, albeit at the cost of some awkwardness.

> Having "per environment" files that reference the code accomplishes the same goal without the copy/paste.

I'm still thinking about it, but it still seems more complicated. For example, suppose I want to have prod on one version of my code and dev on another. What code do I reference to see the different versions? Or do I have to switch my `git` repo to different tags? My point is that it's "complected."

At least with the code generation, I can say that this code directly reflects what's deployed.

***

**brikis98** commented *Nov 25, 2016*

I just realized the idea for `spin-up` won't work. At least not as written in the original description of this issue.

The reason is that the `terraform_remote_state` resources may use Terraform interpolation, referencing variables, data sources, etc. In other words, Terragrunt cannot just statically parse the `*.tf` files in a folder, figure out the `terraform_remote_state` resources they use, and build a dependency graph from it because that data isn't static. 

It's an odd chicken & egg problem: you don't know what a Terraform template depends on until you run `terraform apply`, but before you run `terraform apply`, you need to know what that template depends on...
***

**brikis98** commented *Nov 25, 2016*

Perhaps we need a simpler solution: a place in the `.terragrunt` file where you list your dependencies.

```hcl
dependencies = {
  paths = ["../vpc", "../data-stores/mysql"]
}

lock = {
   backend = "dynamodb"
   config {
     state_file_id = "terragrunt-test-fixture"
   }
}
```

The `spin-up` command could simply read in the list of `paths` in the `dependencies` block and use that to build up the dependency graph. On the positive side, this is very simple to use and straightforward to implement. The downside is that it's duplicated information. It would be easy to add some dependency to your templates and to forget to express that in the `.terragrunt` file. 
***

**josh-padnick** commented *Nov 28, 2016*

Ah, that's a good point. I was thinking of fancy things terragrunt could do in memory to generate portions of the terraform files in real-time by resolving vars, but I agree that this seems too risky and error-prone to pursue.

Some other ideas: We could "cheat" by reading terraform remote state configuration in the `.terraform` folder, but that will only be available after the initial terraform run. We could have terragrunt run a terraform plan and parse the resulting failure messages to infer dependencies, but that seems too brittle. We could also just defer to convention. If you encode your `data.terraform_remote_state` using static values only, then you can use `terragrunt spin-up`, but if we detect any dynamics vars, terragrunt will error out.

If we do go the route of explicitly listing dependencies in the `.terragrunt` file, perhaps terragrunt should make reference to the `data.terraform_remote_state.my_state` resources that give rise to the dependencies. This way, terragrunt can complain if some but not all dependencies are declared in the `.terragrunt` file. 

My favorite option right now is adopting a convention. How limiting would this be? Do many users compute these dynamically?
***

**brikis98** commented *Nov 28, 2016*

> My favorite option right now is adopting a convention. How limiting would this be? Do many users compute these dynamically?

Well, for one, it wouldn't work for our own templates :) We typically define at least the AWS region in a variable.

I think for the initial implementation, I'm just going to go with an explicit list of dependencies in `.terragrunt`. It's simple and it's better than nothing. We can do some follow-ups to try to improve that later on.
***

