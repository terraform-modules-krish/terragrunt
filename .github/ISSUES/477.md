# Terragrunt should default to env var for remote state config profile

**brikis98** commented *May 14, 2018*

When creating the [S3 client](https://github.com/gruntwork-io/terragrunt/blob/master/remote/remote_state_s3.go#L51), Terragrunt tries to read the AWS region and profile values from the `config`. If those values are not defined by the user, then Go will give back an empty string. As a result, Terragrunt ends up using an empty string as the profile, and does NOT use the `AWS_PROFILE` environment variable as it should. This is a rather frustrating result of the ambuity in Go between default/empty values (`""`) and no value (sometimes `nil`, but sometimes also `""`). 
<br />
***


**ghost** commented *Jun 1, 2018*

For what it is worth, terragrunt also ignores the profile value that is declared in the config block in the root terraform.tfvars file.  In other words, it is default profile or bust.  Which is a big huge bust, since it means you cannot store your terraform state in any account but your default account, so it precludes using the default credentials for any account other than the one used for terraform state.

I feel like this is a new phenomenon, as I could swear I was running without default credentials for a long time.  But I recently updated terragrunt to a version compatible with terraform > 0.10.8 and I feel like that is when it probably started requiring default credentials that work to access the state bucket.  I was onboarding a new developer today, and he has default credentials pointing to a different account entirely, and it took me ages to figure out why it was failing to authenticate when trying to read state.  Weirdly, it seems to use the specified profile when locking the dynamodb lock, since he was able to do that.  But after acquiring the lock, it would error out the first time it tries to populate a terraform_remote_state data source.  I don't have a profile declared in my templates and modules. I was counting on terragrunt merging those in from the terragrunt remote_state block in the top level terraform.tfvars 

```
terragrunt = {
  remote_state {
    backend = "s3"

    config {
      bucket = "xxxxxxxxxxxxx"
      key    = "${path_relative_to_include()}/terraform.tfstate"
      region = "us-east-1"
      encrypt = true
      dynamodb_table = "terraform-lock-table"
      profile = "terraform_state"
    }
  }

  terraform {
    ...
  }
}
***

**brikis98** commented *Jun 1, 2018*

> But after acquiring the lock, it would error out the first time it tries to populate a terraform_remote_state data source. I don't have a profile declared in my templates and modules. I was counting on terragrunt merging those in from the terragrunt remote_state block in the top level terraform.tfvars

The profile setting you put into the `config` of the `remote_state { ... }` block *only* applies to remote state. Terragrunt does not apply it to your entire Terraform configuration. This behavior is consistent with Terraform itself and the `backend` configuration.
***

**ghost** commented *Jun 1, 2018*

Yes, I am aware of that.  I have a profile for remote_state (called
terraform_state) and a DIFFERENT profile (terraform) for use by the aws
provider in my templates.  The aws provider does pick up the correct
profile when specified in the provider declaration in a template.  The
remote_state thing simply doesn't work unless my default credentials are
set up as copies of the credentials in my terraform_state profile, or some
other credentials with equivalent authorization in the same account.

On Thu, May 31, 2018 at 7:47 PM, Yevgeniy Brikman <notifications@github.com>
wrote:

> But after acquiring the lock, it would error out the first time it tries
> to populate a terraform_remote_state data source. I don't have a profile
> declared in my templates and modules. I was counting on terragrunt merging
> those in from the terragrunt remote_state block in the top level
> terraform.tfvars
>
> The profile setting you put into the config of the remote_state { ... }
> block *only* applies to remote state. Terragrunt does not apply it to
> your entire Terraform configuration. This behavior is consistent with
> Terraform itself and the backend configuration.
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/gruntwork-io/terragrunt/issues/477#issuecomment-393742391>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AdYOqG2L9t5PmTL4X1GexFodDWUhEBC9ks5t4KtGgaJpZM4T9A_4>
> .
>

***

**ghost** commented *Jun 1, 2018*

I haven't tried to see what happens if I explicitly put a profile value in
each terraform_remote_state data source that exists in my templates.  I was
under the impression that the config block in the top level
terraform.tfvars file would be merged with whatever is in the
terraform_remote_state config block.  But it seems that the profile is not
merged, or else the merged value is not used by terraform.  All I know is
that I cannot access my remote_state, no matter what value I put in the
profile in the config block at the top level, unless my default credentials
are set up to have access.  This could be a problem with
terraform_remote_state doing a really poor job of instantiating the AWS
provider it then uses to load state, or it could be that terragrunt is not
properly setting the profile the way it should be.  Either way, the system
does not work as advertised and it isn't possible to use a non-default
credential to access state stored in S3.

On Thu, May 31, 2018 at 9:57 PM, Samuel Gendler <sam@stem.is> wrote:

> Yes, I am aware of that.  I have a profile for remote_state (called
> terraform_state) and a DIFFERENT profile (terraform) for use by the aws
> provider in my templates.  The aws provider does pick up the correct
> profile when specified in the provider declaration in a template.  The
> remote_state thing simply doesn't work unless my default credentials are
> set up as copies of the credentials in my terraform_state profile, or some
> other credentials with equivalent authorization in the same account.
>
> On Thu, May 31, 2018 at 7:47 PM, Yevgeniy Brikman <
> notifications@github.com> wrote:
>
>> But after acquiring the lock, it would error out the first time it tries
>> to populate a terraform_remote_state data source. I don't have a profile
>> declared in my templates and modules. I was counting on terragrunt merging
>> those in from the terragrunt remote_state block in the top level
>> terraform.tfvars
>>
>> The profile setting you put into the config of the remote_state { ... }
>> block *only* applies to remote state. Terragrunt does not apply it to
>> your entire Terraform configuration. This behavior is consistent with
>> Terraform itself and the backend configuration.
>>
>> —
>> You are receiving this because you commented.
>> Reply to this email directly, view it on GitHub
>> <https://github.com/gruntwork-io/terragrunt/issues/477#issuecomment-393742391>,
>> or mute the thread
>> <https://github.com/notifications/unsubscribe-auth/AdYOqG2L9t5PmTL4X1GexFodDWUhEBC9ks5t4KtGgaJpZM4T9A_4>
>> .
>>
>
>

***

**brikis98** commented *Jun 1, 2018*

`terraform_remote_state` does not use the same settings as the `backend` in Terraform (or `remote_state` in Terragrunt). It uses the settings from the `provider` in the Terraform code. 
***

**ghost** commented *Jun 1, 2018*

No, it doesn't.  If it used the provider in the template, it would fail
every time for me.  It declares its own provider, using the backend from
terraform or remote_state in terragrunt.  That's why it has a config
block.  How would it work at all if it used different credentials when
writing remote_state than it uses to read it?

See the docs for the s3 backend here:
https://www.terraform.io/docs/backends/types/s3.html

It explicitly mentions terraform_remote_state as being the consumer of the
backend config: "To make use of the S3 remote state we can use the
terraform_remote_state data source."

Surely this is exactly what terragrunt's override of the backend is for?
There's a terraform block with an empty s3 backend block in every
template.  The expectation is that the s3 backend block is somehow
filled-in via the config provided in the terragrunt block in my top-level
terraform.tfvars file.  It has to write the backend config for both writes
AND reads or it would be pretty darned useless - as it is, if you have
non-default credentials, since it currently only works for writing to the
backend, not for reading from it.

I think perhaps even you weren't aware of this, so you are only rewriting
the backend config block up at the top of the file, rather than rewriting
that block AND any config block within terraform_remote_state data
sources.  This would certainly explain why I have to make my default
credentials equal to those I want t_r_s to use.

On Thu, May 31, 2018 at 10:08 PM, Yevgeniy Brikman <notifications@github.com
> wrote:

> terraform_remote_state does not use the same settings as the backend in
> Terraform (or remote_state in Terragrunt). It uses the settings from the
> provider in the Terraform code.
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/gruntwork-io/terragrunt/issues/477#issuecomment-393759258>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AdYOqBemdIiQaCtrgTHcxWsPRkqjCTdlks5t4MxFgaJpZM4T9A_4>
> .
>

***

**brikis98** commented *Jun 1, 2018*

I think some clarification is in order:

1. The `backend` configuration determines where the state file (`.tfstate`) for the current Terraform module is stored. Terragrunt's `remote_state` block solely fills in this `backend` configuration.
1. To store the state file in, say, an S3 bucket, you need credentials. The `backend` config reads its credentials from the environment (e.g., `AWS_ACCESS_KEY_ID`) and/or from parameters in the `backend` config itself (e.g., the `profile` param).
1. The `terraform_remote_state` data source can be used to read *another* `.tfstate` file. The config you put in it is not read from the `backend` configuration, as it wouldn't really make sense to read the `.tfstate` file from the current module. 
1. To read the state file from, say, an S3 bucket, you need credentials. The `terraform_remote_state` data source, like all other data sources and resources, uses the credentials from the relevant `provider` defined in your Terraform code. The `backend` config in the current module, and Terragrunt, have no influence on this.
***

**ghost** commented *Jun 1, 2018*

You are correct, except for point #4.  It absolutely does NOT use the aws
provider in the terraform template.  It instantiates an aws provider using
values from the config block of the terraform_remote_state in EXACTLY the
same way that the remote_state does when writing the remote state file.
That is why you have to provide the same config elements to the t_r_s
config block as you do to an aws provider or a remote_state s3 backend. It
uses an independent provider, and it cannot be told to simply use a
particular provider by alias, the way you can other resources and data
sources. Additionally, there was (and possibly still is) a bug in that code
in terraform, which causes it to ignore the profile that is set in the
config block.  That was true of terraform 0.10.8, which I was stuck on for
a long while due to the forward incompatibility of the gruntworks library
after terraform shipped a breaking change.  Now that the library modules
are terraform 0.11.x compatible, I hope the profile problem in t_r_s is
fixed, but I haven't confirmed it.

I chimed in here, because I had forgotten the details of why profile didn't
work in t_r_s, and I got confused when I read this issue, but it really
would make a lot of sense for terragrunt to override the settings (other
than key) in the t_r_s config block with whatever it is overriding values
with in remote_state config, because the read has to happen from the
correct location, and that location was set by terragrunt's original
override.  The key declared in the template should be preserved, since the
template has no way to know what the path to the original state was, since
that varies by the path to the calling terraform.tfvars file.  But bucket,
profile, region, backend, credentials, etc - those should all be inherited
from the same place that the state writer gets them - the backend config in
the terragrunt block - since the reader needs to be configured in the same
way as the writer, except in the rare condition that you are transitioning
from one backend to another, in which case, the reader needs one config and
the writer needs a different one.

The one saving grace to the complexity is that it is possible to do
variable replacement in a t_r_s declaration, so you can mostly parameterize
the read-side and generate that config dynamically.  The write-side truly
requires terragrunt's intervention because it is otherwise unwilling to be
dynamically generated, so cannot be parameterized without terragrunt's
re-writing.

You end up with something like this in top level:

```
terragrunt = {
  remote_state {
    backend = "s3"

    config {
      bucket = "xxxxxxxxxxxxxxx"
      key    = "${path_relative_to_include()}/terraform.tfstate"
      region = "xxxxxxx"
      encrypt = true
      dynamodb_table = "xxxxxxxxxxxxx"
      profile = "xxxxxxxxxx"
    }
  }

# declare vars to help write t_r_s config blocks here.  These vars cannot
be used in terragrunt block
terraform_state_region = "xxxxxxx"
terraform_state_bucket = "xxxxxxxxxxxxxxx"
terraform_state_profile = "xxxxxxxxxxx"
```

And then wherever you use terraform_remote_state, it has to look something
like this:

```
data "terraform_remote_state" "template1" {
  backend = "s3"
  config {
    region = "${var.terraform_state_region}"
    key    =
"${var.aws_account_id}/${var.aws_region}/${var.environment}/${var.template_name}/terraform.tfstate"
    bucket = "${var.terraform_state_bucket}"
    profile = "${var.terraform_state_profile}"
  }
}
```

Note that it is ostensibly identical to the terragrunt.remote_state block,
but without the dynamo_db_table declaration, and with region, bucket, and
profile sourced from the vars we set in the top level terraform.tfvars file
instead of getting the values from the terragrunt.remote_state block.  But
if you change which backend terragrunt configures for remote_state, or
modify the config, you have to make separate changes to update every single
t_r_s referencde in the codebase.  I've reduced the difficulty by
paramaterizing it some, but it would be better to get it from the
terragrunt block automatically.

If you don't believe me that t_r_s instantiates its own provider from its
own config block, try changing the credentials of the provider in the
template, but don't change your default credentials.  You'll see that t_r_s
does not use the configured provider, it uses the default credentials
(unless it has credentials explicitly configured in the config block).  You
aren't seeing this, I guess, because you always use just one set of
credentials and they are also your default credentials, so you cannot
diffeentiate between one and the other - the same problem the terraform
devs must have had when they failed to detect that the t_r_s internal
provider wasn't using profile correctly.

Or look at the code - the terraform_remote_state code is in
terraform_providers_terraform:
https://github.com/terraform-providers/terraform-provider-terraform/blob/master/terraform/data_source_state.go
Which initializes the backend here:
https://github.com/hashicorp/terraform/blob/master/backend/init/init.go
Which leads to the s3 backend here:
https://github.com/hashicorp/terraform/blob/master/backend/remote-state/s3/backend.go

The terraform_remote_state data source is part of the terraform provider,
not the aws provider, which is why it doesn't automatically inherit access
to the declared aws provider and must instead instantiate its own, via the
config block.



On Thu, May 31, 2018 at 10:55 PM, Yevgeniy Brikman <notifications@github.com
> wrote:

> I think some clarification is in order:
>
>    1. The backend configuration determines where the state file (.tfstate)
>    for the current Terraform module is stored. Terragrunt's remote_state
>    block solely fills in this backend configuration.
>    2. To store the state file in, say, an S3 bucket, you need
>    credentials. The backend config reads its credentials from the
>    environment (e.g., AWS_ACCESS_KEY_ID) and/or from parameters in the
>    backend config itself (e.g., the profile param).
>    3. The terraform_remote_state data source can be used to read *another*
>    .tfstate file. The config you put in it is not read from the backend
>    configuration, as it wouldn't really make sense to read the .tfstate
>    file from the current module.
>    4. To read the state file from, say, an S3 bucket, you need
>    credentials. The terraform_remote_state data source, like all other
>    data sources and resources, uses the credentials from the relevant
>    provider defined in your Terraform code. The backend config in the
>    current module, and Terragrunt, have no influence on this.
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/gruntwork-io/terragrunt/issues/477#issuecomment-393768451>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AdYOqGjcLtEpXho1dW4aqjBXo2Ylgd_8ks5t4NdMgaJpZM4T9A_4>
> .
>

***

**brikis98** commented *Jun 1, 2018*

Ah, you're right. It's `terraform_remote_state`, not `aws_remote_state`, after all :)


***

**ghost** commented *Jun 1, 2018*

Yeah, I had that exact same realization back when i first uncovered the bug
with how t_r_s instantiates the provider incorrectly.

I think a case could honestly be made for having terragrunt manipulate the
backend and config of t_r_s data sources, though it is also certainly
possible to live without it.  Somehow, despite moving to explicitly adding
a profile = "${var.my_state_profile}" in t_r_s, the one that ended up
serving as a model for cut and paste all over my codebase shortly
thereafter was lacking it, so I was missing an explicit profile pretty much
everywhere.  I'm still not sure if specifying a profile is fixed, since I
do have default credentials set up at the moment, and I'm too busy with a
release to really dig in at the moment, but I suspect, from things my
recently onboarded developer crew have mentioned, that an explicit profile
probably does work when it is actually specified, and I updated all of our
t_r_s references to include a profile last night.

But it would be way cooler to be able to just specify the config.key in
each t_r_s and have backend and the rest of config populated with the
values from the terragrunt.remote_state.

Combined with my module which knows how to populate the various path
elements in the key according to how various flags are set, it would make
getting t_r_s from the correct place in every template occupy a whole lot
less space in each template.

Throw in code which auto-populates dependency paths to match what it finds
in each t_r_s, and I'd be a very happy camper.

Solve the problem of copying inputs and outputs with identical names
between templates and modules and you'd probably reduce the amount of code
in my infrastructure repos by more than 50%.  And it is the most
error-prone 50%, too, since it tends to be rife with copy/paste errors.


On Fri, Jun 1, 2018 at 9:58 AM, Yevgeniy Brikman <notifications@github.com>
wrote:

> Ah, you're right. It's terraform_remote_state, not aws_remote_state,
> after all :)
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/gruntwork-io/terragrunt/issues/477#issuecomment-393943470>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AdYOqKK5k7Bnc7vcS7EtfEHdW5b25iJgks5t4XLCgaJpZM4T9A_4>
> .
>

***

**brikis98** commented *Jun 1, 2018*

Terragrunt is able to fill in the parameters of the `backend` config because Terraform exposes CLI parameters (`-backend-config`) for exactly this purpose. There is no built-in equivalent for setting configs in `terraform_remote_state`, so we'd have to come up with our own design for that.

What we do today, and it sounds like you're doing it to, is have each module expose variables for the settings that module needs in the `config` of its `terraform_remote_state` data sources:

```hcl
data "terraform_remote_state" "vpc" {
  backend = "s3" 
  config {
    bucket = "${var.terraform_remote_state_bucket}"
    key    = "${var.terraform_remote_state_key}"
    region = "${var.terraform_remote_state_region}"
  }
}
```

You can then use normal Terraform/Terragrunt variable management (e.g., `.tfvars` files, `-var` flags, etc) to set those parameters. This of course has downsides, as you have to write a lot of this repetitive `terraform_remote_state` code and set variables all over the place.

What would the alternative you want to see look like? For example, you mentioned only having to specify `config.key`, so presumably, something like this:

```hcl
data "terraform_remote_state" "vpc" {
  config {
    key = "${var.terraform_remote_state_key}"
  }
}
```

But how would Terragrunt set the other parameters? If there are no variables exposed, it can't set them via `-var` arguments or `.tfvars` files. Scanning the code, parsing it, and modifying it on the fly sounds every error prone. Moreover, how would Terragrunt know which values to put in there? Sure, in some cases, the other values (bucket, region, etc) would be the same as what's in the `remote_state` Terragrunt config, but in some cases, you may be reading state from totally different buckets. How would that be configured?
***

**ghost** commented *Jun 1, 2018*

If there really is a use-case that would use other buckets than where you
are writing your state to, then that would definitely be problematic.  But
I'm not really sure how that would work, since running terragrunt in those
templates would result in state being written to a different place than it
is subsequently being read from.  It seem like, by definition, everything
except the key of the state file pretty much has to match the values in the
remote_state block, since it is reading where remote_state is writing.  As
for how it would do it, I assumed it would do it pretty much the same way
it does it for the backend declaration, which I assumed merges the values
in the terragrunt block with whatever it finds in the template, so that
anything hardcoded into the template overrides what is in the terragrunt
block.

I don't know how you are doing the parsing and replacing.  If it is some
simple regex type thing, that'd probably be pretty hard to get right.  I
kind of assumed you were probably using terraform's parser to parse the
file, then merging a node in memory before using terraform to serialize the
updated graph back out as an updated template.  That seems like the kind of
thing that would be trivial to do in json, yaml, xml, or any of the other
structured markup languages hashicorp didn't use when building terraform,
so I figured it should be fairly easy in their home-grown language, too -
because why would you gave that kind of power up in order to build your own
language?

I've got an empty terraform.backend block in each of my templates:

```
terraform {
  backend "s3" {}
}
```

and I've got this in my top level terraform.tfvars:

```
terragrunt = {
  remote_state {
    backend = "s3"

    config {
      bucket = "xxxx_xxxxxxxx_xxxx"
      key    = "${path_relative_to_include()}/terraform.tfstate"
      region = "us-east-1"
      encrypt = true
      dynamodb_table = "xxxxxxxx-xxxx-xxx"
      profile = "terraform_state"
    }
  }
}
```

I assumed terragrunt was merging that config block into the
terraform.backend block.

So if I have this in my template as well:

```
data "terraform_remote_state" "fnord" {
  backend = "s3"

  config {
    key =
"${module.locate.fnord_account_id}/${module.locate.fnord_region}/${module.locate.fnord_environment}/${module.locate.fnord_template}/terraform.tfstate"
  }
}
```

That you could basically do something like:

```
if trs.fnord.backend = tg.rs.backend {
  merge(tg.rs.backend.config, trs.fnord.config)
}
```

Or, since the backend is unlikely to change very often, why not make the
default behaviour assume that the backends match if not otherwise
specified, so that the t_r_s declaration would be an even simpler:

```
data "terraform_remote_state" "fnord" {
  config {
    key =
"${module.locate.fnord_account_id}/${module.locate.fnord_region}/${module.locate.fnord_environment}/${module.locate.fnord_template}/terraform.tfstate"
  }
}
```

Frankly, even a regex-style replacement of the block doesn't seem like it
would be too hard to get right, but those kinds of things do often have
lots of weird corner cases once users start actually using it.



***

**brikis98** commented *Jun 1, 2018*

> If there really is a use-case that would use other buckets than where you are writing your state to, then that would definitely be problematic.  But I'm not really sure how that would work, since running terragrunt in those templates would result in state being written to a different place than it is subsequently being read from. 

Simple example:

* I have on AWS account for each team: `team-foo`, `team-bar`, `team-baz`.
* Each team stores its Terraform state files in a bucket in its own account: `terraform-state-foo`, `terraform-state-bar`, `terraform-state-baz`.
* However, there's also one more account: `common`. This one manages things common across all teams, such as domain names in Route 53.
* The Terraform code for each of those teams uses `terraform_remote_state` to pull state from its own account (e.g., `terraform-state-bar`), but *also* the state from the `common` account (e.g., `terraform-state-common`). For example, the team needs state from the `common` account to get a Route 53 Hosted Zone ID.

> I don't know how you are doing the parsing and replacing.

Terragrunt does not do any parsing or replacing. As I said above, the way it provides the `remote_state` settings to your Terraform code is via the `-backend-config` CLI params that Terraform exposes precisely for that purpose. So no parsing or code generation or anything is necessary for `backend` configuration. Unfortunately, no such CLI flags exist for `terraform_remote_state`.

> That seems like the kind of thing that would be trivial to do in json, yaml, xml, or any of the other structured markup languages hashicorp didn't use when building terraform, so I figured it should be fairly easy in their home-grown language, too - because why would you gave that kind of power up in order to build your own language?

Terraform uses the [hcl library](https://github.com/hashicorp/hcl) for parsing. I've tried it, and unfortunately, while it works fine for reading the code, it [does not support modifying and writing the code back out](https://github.com/hashicorp/hcl/issues/201). This may change with HCL2 and its parser.

Moreover, parsing the code is non-trivial, as `terraform_remote_state` definitions could be not only in the top-level `.tf` files, but in any modules they pull in, and any modules those modules pull in, and so on.

> Frankly, even a regex-style replacement of the block doesn't seem like it would be too hard to get right, but those kinds of things do often have lots of weird corner cases once users start actually using it.

Haha, no, regex is definitely not the right tool for this job ([the center cannot hold...](https://stackoverflow.com/a/1732454/483528)).
***

**ghost** commented *Jun 1, 2018*

We actually do something much like that, but all teams write to the same bucket, precisely because coordinating multiple buckets seemed like a PITA.  It's a security hole, in that state files tend to have secrets, but I'm solving that by putting prod entirely in its own account. It doesn't share resources with anything.  Nearly all of the lower environments share some resources (staging is mostly an exact structural copy of prod, but downsized), but everything writes to the same bucket since they are all in the same git repo and have the same top-level terraform.tfvars file, so I don't have that issue.

Not having real support for modifying nodes via the language parser is pretty much a dead-end, regardless of the bucket thing.  I didn't know you were using a convenient cli argument for configuring the remote state, so I figured you must have already solved the problem of recursively descending through everything and finding all backend blocks that needed to be updated.  If you had done that, it wouldn't have been such a stretch to imagine rewriting the terraform_remote_state config blocks in the same manner, as well.  It would be a relatively minor extension of the existing mechanism.  It probably still could be implemented as a fairly minor extension of the existing mechanism, but it would have to be done in terraform and exposed to terragrunt via more cli args, just as it currently is for remote_state.  You could probably even manage switching the buckets out dynamically via a value left in the 'dummy' config that would be analogous to the empty backend block it makes you write, now.
***

