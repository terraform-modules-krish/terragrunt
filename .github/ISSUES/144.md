# Problem with AWS Profiles using Terragrunt

**knaak** commented *Feb 22, 2017*

My company requires us to get a new session token/access id/access key every hour.  In order to accommodate this, we have a script that adds/updates a profile to the credentials file like this:

```
[maas-non-prod]
output = json
region = us-east-1
aws_access_key_id = removed
aws_secret_access_key = removed
aws_session_token = removed
```

The only consistent thing is the profile name, which I guess is safe.  So, within Terragrunt I specify which profiles to use:

```
lock = {
	backend = "dynamodb"
	config {
 	    aws_profile = "maas-non-prod"
... removed stuff
	}
}

remote_state = {
	backend = "s3"
	config {
	    profile = "maas-non-prod"
... removed stuff
	}
}
```

Aside from the inconsistent profile name, it seems to work.  In my terraform scripts, I also pass profile:

```
provider "aws" {
    region = "${var.region}"
	profile = "${var.profile}"
}
```

So I think that is all fine.

If I run: ```terraform plan```.  It works fine.  If I run: ```terragrunt plan``` I receive an error that it can't find access key and that error appears to come from terraplan not terragrunt.

```
â†’ terragrunt plan
[terragrunt] 2017/02/22 16:10:45 Reading Terragrunt config file at /Users/knaak/Developer/maas/terraform/api_gateway/services/news_service/staging/.terragrunt
[terragrunt] 2017/02/22 16:10:45 DEPRECATION WARNING: Found deprecated config file format /Users/knaak/Developer/maas/terraform/api_gateway/services/news_service/staging/.terragrunt. This old config format will not be supported in the future. Please move your config files into a terraform.tfvars file.
[terragrunt] 2017/02/22 16:10:45 Running command: terraform get -update
Get: file:///Users/knaak/Developer/maas/terraform/api_gateway/services/news_service (update)
Get: file:///Users/knaak/Developer/maas/terraform/api_gateway/services (update)
Get: file:///Users/knaak/Developer/maas/terraform/api_gateway/modules (update)
Get: file:///Users/knaak/Developer/maas/terraform/api_gateway/modules (update)
[terragrunt] 2017/02/22 16:10:45 Initializing remote state for the s3 backend
[terragrunt] 2017/02/22 16:10:46 Configuring remote state for the s3 backend
[terragrunt] 2017/02/22 16:10:46 Running command: terraform remote config -backend s3 -backend-config=profile=maas-non-prod -backend-config=encrypt=true -backend-config=bucket=maas-terraform-state-non-prod -backend-config=key=/api-gateway/news_services/staging/terraform.tfstate -backend-config=region=us-east-1
Remote state management enabled
Remote state configured and pulled.
[terragrunt] 2017/02/22 16:10:48 Running command: terraform plan
Error reloading remote state: AuthorizationHeaderMalformed: The authorization header is malformed; a non-empty Access Key (AKID) must be provided in the credential.
	status code: 400, request id: 8BFDE1A0FD2692A1
exit status 1
[terragrunt] 2017/02/22 16:10:49 exit status 1
```

specifically this part:
```
Error reloading remote state: AuthorizationHeaderMalformed: The authorization header is malformed; a non-empty Access Key (AKID) must be provided in the credential.
	status code: 400, request id: 8BFDE1A0FD2692A1
```

looks like a terraform error, not a terragrunt.  As I mentioned, if I run terraform plan on the same project, it works fine.

Any ideas?


<br />
***


**rexc** commented *Feb 23, 2017*

I think there is a general inconsistency with profiles. I have something simliar when I tried to use AWS profile like this, where I assume a role into another AWS account:
```
[terragrunt]
role_arn= arn:<blah>
source_profile=default
```
and terraform:
```
 Configure Terragrunt to automatically store tfstate files in an S3 bucket
  remote_state {
    backend = "s3"
    config {
      encrypt = "true"
      bucket = "s3-terragrunt-states"
      key = "development/terraform.tfstate"
      region = "ap-southeast-2"
      profile = "terragrunt"
      }
  }
```

I found that bucket was created in the AWS account as default profile but access would use my terragrunt profile credentials which didn't have access to the bucket which should have been created in the same AWS account as the terragrunt credentials.
***

**brikis98** commented *Feb 23, 2017*

@Bowbaq I believe you worked on some of the AWS profile stuff. Any ideas what could be causing this?
***

**Bowbaq** commented *Feb 23, 2017*

For the first problem, sounds like https://github.com/hashicorp/terraform/issues/2774 which is completely unrelated.

As for the second problem, I'm not sure what could be going on. The profile is set explicitly if present in the config. That said, given the first error, perhaps the bucket was already created before `terragrunt` ran.
***

**knaak** commented *Feb 27, 2017*

@Bowbaq I believe you are correct.  As per #2774, I arbitrarily changed the name of the bucket to something unique, created that bucket from the aws account I wanted to use, updated the terragrunt property file, removed all other references to any other account in my aws credentials file, and re-ran terragrunt and it worked.

I've tried to confuse terragrunt by adding [default] profile, and my original accounts back into credentials file, but I can't reproduce.




***

