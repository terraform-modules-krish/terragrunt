# Either the target module has not been applied yet, or the module has no outputs

**kiklop74** commented *Nov 12, 2021*

I have a project structure like this:

```
├── awsmodules
│   ├── mod1
│   │   ├── main.tf
│   │   ├── outputs.tf
│   │   └── variables.tf
│   ├── mod2
│   │   ├── main.tf
│   │   ├── outputs.tf
│   │   └── variables.tf
│   ├── mod3
│   │   ├── main.tf
│   │   ├── outputs.tf
│   │   └── variables.tf
├── stack-base
│   ├── mod1
│   │   └── terragrunt.hcl
│   ├── mod2
│   │   └── terragrunt.hcl
│   ├── mod3
│   │   └── terragrunt.hcl
│   ├── terragrunt.hcl

```

awsmodules contains stock terraform modules. Later these are referred with their terragrunt counterparts.

mod1 and mod2 are independent but mod3 depends on them. I am getting the "Either the target module has not been applied yet, or the module has no outputs" during apply.

```terraform
# terragrunt mod3
include {
  path = find_in_parent_folders()
}

terraform {
  source = "../../awsmodules//mod3"
}

inputs = {
  foo = dependency.mod1.outputs.foo
  faa = dependency.mod2.outputs.faa
}

dependency "mod1" {
  config_path = "../mod1"
  mock_outputs = {
    foo = "foo"
  }
  mock_outputs_allowed_terraform_commands = ["validate"]
}

dependency "mod2" {
  config_path = "../mod2"
  mock_outputs = {
    faa = "foo"
  }
  mock_outputs_allowed_terraform_commands = ["validate"]
}
```

The execution is done in quite specific way as I want to avoid polluting the terragrunt directories with temporary data so I run it from separate directory no within the root folder of terraform/terragrunt code.

```sh
  terragrunt run-all apply \
  --terragrunt-download-dir '/var/tmp/fccache' \
  --terragrunt-non-interactive \
  --terragrunt-working-dir "/path/to/tgcode" \
  --terragrunt-log-level 'trace' \
  --terragrunt-source-update

```

I am using the local backend and AWS provider. Terrraform 1.08 and terragrunt 0.35.9

How to determine the issue? Are relative paths bad idea? Should one execute terragrunt from within the terragrunt directory, does that matter?


<br />
***


**denis256** commented *Nov 12, 2021*

Hi, maybe make sense in `mod3` to declare `mock_outputs` from `mod1` and `mod2`?
I prepared example project with similar layout in:
https://github.com/denis256/terragrunt-tests/tree/master/module-dependencies

https://terragrunt.gruntwork.io/docs/features/execute-terraform-commands-on-multiple-modules-at-once/#unapplied-dependency-and-mock-outputs
***

**kiklop74** commented *Nov 12, 2021*

Unfortunately mocks do not resolve my problem. I already have mocks set only for validate because otherwise they appear in the plan and generate other kind of errors due to dummy data. And I will update the text accordingly.
***

**kiklop74** commented *Nov 12, 2021*

Is the structure I'm proposing viable at all? What are the alternatives?
***

**kiklop74** commented *Nov 12, 2021*

I have this terraform module called vpc , real code full source:

```terraform
# maint.tf
data "aws_vpc" "main" {
  default = true
}

data "aws_subnets" "vpcsubnets" {
  filter {
    name   = "vpc-id"
    values = [data.aws_vpc.main.id]
  }
}

data "aws_security_group" "uh_aws_internal" {
  filter {
    name   = "vpc-id"
    values = [data.aws_vpc.main.id]
  }
  filter {
    name   = "group-name"
    values = ["uh-aws-internal"]
  }
}

data "aws_security_group" "uh_web_bastion" {
  filter {
    name   = "vpc-id"
    values = [data.aws_vpc.main.id]
  }
  filter {
    name   = "group-name"
    values = ["uh-web-bastion"]
  }
}

data "aws_security_group" "uh_egress_allow_all" {
  filter {
    name   = "vpc-id"
    values = [data.aws_vpc.main.id]
  }
  filter {
    name   = "group-name"
    values = ["uh-egress-allow-all"]
  }
}

data "aws_subnets" "default" {
  filter {
    name   = "vpc-id"
    values = [data.aws_vpc.main.id]
  }
  filter {
    name   = "default-for-az"
    values = [true]
  }
  filter {
    name   = "state"
    values = ["available"]
  }
}

data "aws_subnet" "vpcsubnet" {
  for_each = toset(data.aws_subnets.default.ids)
  id       = each.value
}

locals {
  filtered_subnets1 = [
    for v in data.aws_subnet.vpcsubnet : v if v.available_ip_address_count > 20
  ]
  filtered_subnets2 = [
    for v in data.aws_subnet.vpcsubnet :
    v if !contains([
      "us-west-2d", "ca-central-1d", "sa-east-1b"
    ], v.availability_zone)
  ]
  filtered_subnets = tolist(
    setintersection(
      local.filtered_subnets1,
      local.filtered_subnets2
    )
  )
  final_subnet = element(local.filtered_subnets, 0)
  config_content = jsonencode({
    default_subnet_id         = local.final_subnet.id
    default_availability_zone = local.final_subnet.availability_zone
  })
}
```

```terraform
# output.tf
output "arn" {
  description = "ARN of the listener"
  value       = data.aws_vpc.main.arn
}

output "id" {
  description = "ID of the listener"
  value       = data.aws_vpc.main.id
}

output "subnets_all" {
  description = "All available subnets"
  value       = data.aws_subnets.vpcsubnets.ids
}

output "default_subnet" {
  description = "Default available subnet"
  value       = local.final_subnet
}

output "default_subnet_configtpl" {
  description = "Default configuration"
  value       = local.config_content
}

output "balancer_groups" {
  description = "Default configuration"
  value = [
    data.aws_security_group.uh_aws_internal.id,
    data.aws_security_group.uh_web_bastion.id
  ]
}

output "uh_aws_internal" {
  value = data.aws_security_group.uh_aws_internal
}

output "uh_web_bastion" {
  value = data.aws_security_group.uh_web_bastion
}

output "uh_egress_allow_all" {
  value = data.aws_security_group.uh_egress_allow_all
}
```

I have vpc terragrunt module referencing this and than use that vpc as dependency in other places. 

```terraform
# stack-base/vpc/terragrunt.hcl
include {
  path = find_in_parent_folders()
}

terraform {
  source = "../../awsmodules//vpc"
}
```

```terraform

# stack-base/webserver/terragrunt.hcl

include {
  path = find_in_parent_folders()
}

terraform {
  source = "../../awsmodules//webserver"
}

inputs = {
  subnet_id           = dependency.vpc.outputs.default_subnet.id
  security_groups     = [
    dependency.vpc.outputs.uh_web_bastion.id,
    dependency.vpc.outputs.uh_aws_internal.id,
    dependency.vpc.outputs.uh_egress_allow_all.id
  ]
}

dependency "vpc" {
  config_path  = "../vpc"
  mock_outputs = {
    default_subnet      = {
      id = "foo"
    }
    uh_web_bastion      = {
      id = "foo"
    }
    uh_aws_internal     = {
      id = "foo"
    }
    uh_egress_allow_all = {
      id = "foo"
    }
  }
  mock_outputs_allowed_terraform_commands = ["validate"]
}

```

so vpc module does not create anything just fetches existing data. Do I need to integrate this and not make it separate module?
 

***

**kiklop74** commented *Nov 12, 2021*

It appears that the main problem was related to the use of `--terragrunt-source-update` parameter. When used it caused the state file to be deleted for the dependency and than not recreate it...
***

**kiklop74** commented *Nov 15, 2021*

Closing this as I have it working, though it would be nice to know if `terragrunt-source-update` should behave like this.
***

