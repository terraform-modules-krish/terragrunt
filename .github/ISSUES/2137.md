# run-all with destroy creates incorrect dependency graph

**OliverKrukowPwC** commented *Jun 9, 2022*

I am experiencing the issue that `terragrunt run-all plan -destroy` and `terragrunt run-all apply -destroy` seem to create an incorrect dependency graph. The dependency graph seems to be the same as for creation (i.e. as if run without the `-destroy` option), instead of the other way around.


**Example**

Structure:
/layer-1
└ terragrunt.hcl
└ /layer-2
   └ terragrunt.hcl

As indicated by the structure, `layer-2` includes a `dependency` on `layer-1`.
The sequence created for destruction is `layer-1` -> `layer-2` - just as in case of creation - instead of `layer-2` -> `layer-1`.

When applied, this results in an error when attempting to destroy `layer-2`, because `layer-1` is then already gone at that point and any inputs used from `layer-1` in `layer-2` are no longer available, so Terraform cannot generate a plan for destruction.

Using terragrunt version 0.35.13.
<br />
***


**hayunofek** commented *Jun 26, 2022*

I would like to add to this scenario. 

I also believe that terragrunt calculates the dependencies wrong on "destruction". 

Let's take the following scenario:
└ moduleA
└─ pls.tf
└ moduleB
└─ pe.tf

in moduleA/pls.tf:
```
resource "private_link_service" "pls" {
...
}
```

in moduleB/pe.tf:
```
resource "private_endpoint" "pe" {
   input = dependency.pls.outputs.pls # Not actual code, but it is what happens behind the scenes
}
```

So what we have here is a module creating a resource (for example Private Link Service) and another module creating a different resource dependent on the first resource (let's say Private Endpoint). 

On creation, it works well. But on destruction, I would expect terragrunt to know how to calculate the depenedencies, and try to destroy moduleB/pe.tf before it tries to destroy moduleA/pls.tf. However, the opposite happens.


***

**levkohimins** commented *Aug 7, 2023*

Hi @OliverKrukowPwC, @hayunofek,
I tried to reproduce the issue and it looks like it's already been resolved. Can you confirm this?
***

**levkohimins** commented *Aug 8, 2023*

I'm closing the issue, but if the issue still exists, feel free to re-open, we'll figure it out.
***

