# How do you handle ephemeral environments based on branches

**sauln** commented *Feb 9, 2021*

Hi all! I've scoured the docs, old issues, and stack overflow comments, but haven't found a pattern for what we're trying to solve.

Our development workflow (when we used Terraform) was to create a new branch, create a new workspace, and develop test on those resources before merging the main and then destroying those resources.  

How would you handle this in Terraform?  We have been able to replace all of our standing workspaces with directories/terragrunt.hcl files as is recommended, but what about ones that only stick around for a bit?

If we added a new directory and terragrunt config file, we would get another namespaced environment, but when we make changes to the modules, it would affect both the standard resources we don't want to modify and the resources we're updating.

The only thing I can think of is to manually edit the state key and add some new key (similar to how we used to use `terraform.workspace`) as argument to the modules. This will make it easy to spin up new resources, but also easy to forget about destroying them. 

Any tips or patterns that you know to support this use case?  Or, very possibly, do you suggest we find a new way of operating?  I'm open to suggestions.
<br />
***


**yorinasub17** commented *Feb 10, 2021*

> Our development workflow (when we used Terraform) was to create a new branch, create a new workspace, and develop test on those resources before merging the main and then destroying those resources.

I think I need a concrete example here to understand this workflow. Can you provide the ideal steps of what you would like to do? E.g., what would the flow be like for adding a new subnet to the VPC?
***

**dalgibbard** commented *Feb 10, 2021*

In case it helps, we deploy PRs ephemerally by having a single ```test/us-west-2/test/<stuff>``` tree, where we read in a ```BUILDNAME``` environment variable, and use that to:
* Name the created resources
* Create the statefile

So essentially, each created build uses the same modules, but creates its own statefile and resources based on its branch name.

We can then lookup leftover builds based on statefiles in S3, and call terragrunt destroy for any that are left hanging around for too long.
***

**sauln** commented *Feb 10, 2021*

@dalgibbard  I think that's a bit like what we're thinking.  So that test tree is entirely separate from the main deployment tree? 

@yorinasub17, here's a bit more details on the workflow:
0. Resources are deployed to our environment. Consumers are consuming ðŸ‘ 
1. We want to make some changes to the resources, so we modify some of the terraform to change some configuration.
2. We deploy these changes along side the already deployed system, not affecting the consumers consuming. We can then test out our changes.
3. Now we think our changes are good, so we merge to main and the new resources are deployed. Consumers are now consuming the new stuff.

Previously, we use a tf workspace to deploy a new copy with the changes.  If I were to add an extra `terragrunt.hcl` file that spins up say `test-sauln-feb10`, it would also deploy the changed terraform code to our standard instance.

I think what @dalgibbard makes sense though. We would have to construct a completely separate tree with duplication of some `terragrunt.hcl` files and different state keys.  Not ideal, but it would get us the test envs.
***

**dalgibbard** commented *Feb 10, 2021*

Yeah for "multiple copies of independent stuff", having unique state files and non-conflicting resource names is probably the answer.

We implemented it with environment variables (passed in as part of Jenkins pipeline), but there's probably a few ways to implement it.

I don't think there's particularly a right or wrong way, or even a best way really. Just minimise code dupe, aim for code readability, and you'll end up with something acceptable ðŸ˜„ 
***

**lorengordon** commented *Feb 10, 2021*

I think it is pretty cool idea to have a completely independent, ephemeral deployment of an environment! Using a variable in the tfstate key makes sense to me.

I've had similar setups when using AWS Amplify and its pull-request preview feature. Open a pull request, it creates a separate deployment and builds the app, and you can review it, live, as long as the PR is open. Every push redeploys the app. Close the PR and the preview is torn down. Merge the PR, and it deploys to the main backend/environment, and destroys the preview environment. It's a little clunky in Amplify, and does leave some "state" elements behind that need manual cleanup, but it is really really useful!
***

**sauln** commented *Feb 11, 2021*

@lorengordon that's essentially the workflow that we had setup with terraform workspaces and was a feature we haven't been able to replicate when migrating to terragrunt. While terragrunt did solve some other much more important issues, I would like getting this workflow back, especially as our team grows and we have more need for parallel and isolated deploys. 


***

**lorengordon** commented *Feb 11, 2021*

@sauln I think I'm suggesting it may already by possible, by injecting the branch into the backend key via an ENV, and having your CI populate the ENV...

```
remote_state {
  backend = "s3"

  config = {
    ...
    key = "tfstate/${path_relative_to_include()}/${get_env("GIT_BRANCH")}/terraform.tfstate"
  }

  generate = {
    path      = "backend.tf"
    if_exists = "overwrite"
  }
}
```

***

**sauln** commented *Feb 11, 2021*

Ahhh! yes, I think that could work ðŸ¾   Thank you!  
***

**sauln** commented *Feb 11, 2021*

I suppose this could be closed now.. This pattern should solve the problem.  I'll test it out and possibly reopen the issue if it turns out it doesn't work.

Thanks everyone for thinking through this ðŸ™‡ 
***

**dalgibbard** commented *Feb 11, 2021*

> @sauln I think I'm suggesting it may already by possible, by injecting the branch into the backend key via an ENV, and having your CI populate the ENV...
> 
> ```
> remote_state {
>   backend = "s3"
> 
>   config = {
>     ...
>     key = "tfstate/${path_relative_to_include()}/${get_env("GIT_BRANCH")}/terraform.tfstate"
>   }
> 
>   generate = {
>     path      = "backend.tf"
>     if_exists = "overwrite"
>   }
> }
> ```

Yup, that's what we do :)
***

