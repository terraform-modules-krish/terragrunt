# Terragrunt With Multi-Provider Modules (Kubernetes)

**THC-AND** commented *Sep 24, 2021*

When implementing the AWS-EKS module from the [Terraform Registry](https://github.com/terraform-aws-modules/terraform-aws-eks) using Terragrunt, the module is set up to require two providers: AWS and Kubernetes, as per the module's [example layouts](https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/examples/basic/main.tf). The Kubernetes provider requires inputs from midway through the module application process, as shown below:

```
provider "kubernetes" {
  host                   = data.aws_eks_cluster.cluster.endpoint
  cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority.0.data)
  token                  = data.aws_eks_cluster_auth.cluster.token
  load_config_file       = false
  version = ">= 1.11.1"
}
```

Because Terragrunt calls providers in advance of the module being applied, a blank version of the Kubernetes provider is pre-loaded, causing the module to fail on trying to generate the Kubernetes configmap resource:

```
Error: Post "http://localhost/api/v1/namespaces/kube-system/configmaps": dial tcp [::1]:80: connect: connection refused
```

As the Kubernetes provider defaults to assuming the cluster will be present on localhost. This issue is similar to an ongoing issue with multiple Terraform wrappers that has been noted [on the module's issues page](https://github.com/terraform-aws-modules/terraform-aws-eks/issues/817).

The solutions proposed on the above thread, including aliasing the providers, do not seem to work. Has anyone else encountered similar issues with multi-provider modules?
<br />
***


**yorinasub17** commented *Sep 27, 2021*

The interaction with providers is actually all handled by Terraform, not Terragrunt. Terragrunt only manages the code, but does nothing with setting up the provider itself. My guess is that this is a misconfiguration in the generated Terraform, but hard to be sure without looking at your code. Can you please provide your `terragrunt.hcl` config?

That said, I've actually run into this before, and in most cases the reason is because of polluted environment variables where you have some `KUBE_***` env var defined and the terraform provider uses that to override what's in the code. I would double check your environment to make sure those env vars are not defined.
***

**andrezaycev** commented *Dec 8, 2021*

Resolve for me. Add to eks/terragrunt.hcl
```hcl
generate "provider-local" {
  path      = "provider-local.tf"
  if_exists = "overwrite_terragrunt"
  contents  = <<EOF

    data "aws_eks_cluster" "eks" {
        name = aws_eks_cluster.this[0].id
    }

    data "aws_eks_cluster_auth" "eks" {
        name = aws_eks_cluster.this[0].id
    }

    provider "kubernetes" {
        host                   = data.aws_eks_cluster.eks.endpoint
        cluster_ca_certificate = base64decode(data.aws_eks_cluster.eks.certificate_authority[0].data)
        token                  = data.aws_eks_cluster_auth.eks.token
    }
EOF
}
```
***

**aditya-ambati** commented *Mar 1, 2023*

@andrezaycev  I tried the above solution but it says "aws_eks_cluster.this[0]" is not defined in root module. 
***

**evgenyidf** commented *May 21, 2023*

> @andrezaycev I tried the above solution but it says "aws_eks_cluster.this[0]" is not defined in root module.

use:

```
    data "aws_eks_cluster" "eks" {
        name = module.aws_eks.cluster_id
    }

    data "aws_eks_cluster_auth" "eks" {
        name = module.aws_eks.cluster_id
    }

    provider "kubernetes" {
        host                   = data.aws_eks_cluster.eks.endpoint
        cluster_ca_certificate = base64decode(data.aws_eks_cluster.eks.certificate_authority[0].data)
        token                  = data.aws_eks_cluster_auth.eks.token
    }
```
***

**VRabadan** commented *Sep 20, 2023*

Tried above solution but it's not working. 
Terraform still rings "localhost" instead of the cluster host, despite the state file clearly showing that the host variable is populated with the correct address. 
***

**dekelummanu** commented *Oct 22, 2023*

> > @andrezaycev I tried the above solution but it says "aws_eks_cluster.this[0]" is not defined in root module.
> 
> use:
> 
> ```
>     data "aws_eks_cluster" "eks" {
>         name = module.aws_eks.cluster_id
>     }
> 
>     data "aws_eks_cluster_auth" "eks" {
>         name = module.aws_eks.cluster_id
>     }
> 
>     provider "kubernetes" {
>         host                   = data.aws_eks_cluster.eks.endpoint
>         cluster_ca_certificate = base64decode(data.aws_eks_cluster.eks.certificate_authority[0].data)
>         token                  = data.aws_eks_cluster_auth.eks.token
>     }
> ```

The following worked for me:

```
data "aws_eks_cluster" "eks" {
        name = var.cluster_name
    }

    data "aws_eks_cluster_auth" "eks" {
        name = var.cluster_name
    }

    provider "kubernetes" {
        host                   = data.aws_eks_cluster.eks.endpoint
        cluster_ca_certificate = base64decode(data.aws_eks_cluster.eks.certificate_authority[0].data)
        token                  = data.aws_eks_cluster_auth.eks.token
        exec {
            api_version = "client.authentication.k8s.io/v1beta1"
            command     = "aws"
            # This requires the awscli to be installed locally where Terraform is executed
            args = ["eks", "get-token", "--cluster-name", var.cluster_name]
        }
    }
```
***

