# Can dependent modules reference other modules objects via interpolation?

**boldandbusted** commented *Nov 16, 2017*

Howdy. I'm trying on https://github.com/gruntwork-io/terragrunt#dependencies-between-modules for size, and have a question regarding, for lack of a better term, interpolation "namespaces" and references between them, as seen in this model.

Let's say I have a setup exactly as you have documented in the above URI. If I interpolate resources required between modules (like referencing ${module.vpc....} in the "mysql" module's main.tf), I see errors like this when running "plan-all":

```
module 'vault_cluster': unknown module referenced: vpc 
```

which, in your model, would be thrown when terragrunt runs terraform init in the "mysql" path. Is this just an artifact of running plan-all before *any* tfstate is written? Will I be able to reference via interpolation "vpc" objects in the .tf files of the dependent modules? Is there something I'm missing in how this should work?

Thanks in advance! :)
<br />
***


**josh-padnick** commented *Nov 16, 2017*

This sounds like a Terraform-specific issue. It usually means that even though you're making a call to `${module.vpc.xxx}` in your `maint.tf`, you're missing the following code in that file:

```hcl
module "vpc" {
  ...
}
```

Keep in mind that `terragrunt plan-all` isn't creating links between these modules. It's merely allowing you to run `terraform plan` on many of them at once, in the order declared by the `dependencies` property of the `terragrunt` block in `terraform.tfvars`.

When I run into issues with an `xxx-all` command, I try to run `terragrunt plan` directly on the offending module to see how it behaves then. 

To give you some general guidance around what you're trying to do, each `main.tf` that you're executing is a separate Terraform Configuration and therefore creates a separate Terraform state once it's applied the first time. Most people store this state remotely (e.g. in S3). Now, in your `mysql` module,  you can declare a [terraform_remote_state](https://www.terraform.io/docs/providers/terraform/d/remote_state.html) block which points at the remote state of your VPC module. You can then access the outputs of the VPC module in our `mysql` module with an expression like:

```hcl
${data.terraform_remote_state.vpc.vpc_id}
```
***

**boldandbusted** commented *Nov 17, 2017*

@josh-padnick Thank you! I didn't know about the "data.terraform_remote_state..."! :D
***

**josh-padnick** commented *Nov 17, 2017*

Great! Feel free to close this ticket if your issue is resolved.
***

**boldandbusted** commented *Nov 17, 2017*

I will close this, but I just have one more follow-on question:

Given a structure like so:

`./
./<environment>
./<environment>/vpc/
./<environment>/mysql/`

In ./, I have:

```
terragrunt = {
# Configure Terragrunt to automatically store tfstate files in S3
  remote_state = {
    backend = "s3"
      config {
        encrypt = true
        bucket  = "terraform-state"
        key     = "${path_relative_to_include()}/terraform.tfstate"
        region  = "us-west-2"
        dynamodb_table = "lock-table"
      }
  }
}
```

I want every `<environment>` (prod, dev, staging, etc.) to source this, so I have this in `./<environment>/`:

```
terragrunt = {
  include {
    path = "${find_in_parent_folders()}"
  }
}

```

But, now, I also want to use this model, so I'm trying this in `./<environment>/vpc/`:

```
terragrunt {}
```

and this in `./<environment>/mysql/`:

```
terragrunt = {
  dependencies {
    paths = [ "../vpc" ]
  }
}

```

My final question is this: Where, in the s3 path, would the tfstate be stored, if I have this setup? I don't think I really *want* a separate tfstate for each of these module subdirectories, only one for the whole `./<environment>`, but will Terragrunt (and Terraform) keep the "remote_state" setup for each of the subdirectories (and will "relative to include" mean that it will not add subdirectories to the "key")? How can I accomplish that and keep the model of *-all? Or am I thinking about this wrong?

***

**boldandbusted** commented *Nov 17, 2017*

P.S. One thing I forgot to mention - I thought about having:

```
terragrunt = {
  include {
    path = "${find_in_parent_folders()}"
  }
}

```

in `./<environment>/<module>`, along with the dependencies stanza, but that seemed to me to mean that I was asking for a *separate* tfstate inside the S3 path as described by the "key" stanza in the top-level directory. In this example case, that tfstate path would be (in s3) `./<environment>/<module>`, while I want just a tfstate file in `./<environment>/` in S3. Am I making sense here?
***

**brikis98** commented *Nov 19, 2017*

> My final question is this: Where, in the s3 path, would the tfstate be stored, if I have this setup? I don't think I really want a separate tfstate for each of these module subdirectories, only one for the whole ./<environment>, but will Terragrunt (and Terraform) keep the "remote_state" setup for each of the subdirectories (and will "relative to include" mean that it will not add subdirectories to the "key")? How can I accomplish that and keep the model of *-all? Or am I thinking about this wrong?

I'm a bit confused by your questions, so perhaps it's worth taking a step back and explaining how state works and Terragrunt's role in it.

Whenever you run `terraform`, it records the state of what it did in a `.tfstate` file. By default, that file is stored in the local folder. That means if you run Terraform once in folder `foo` and once in folder `bar`, each of those folders will have a separate `.tfstate` file.

To use Terraform as a team, you need to store that `.tfstate` file in a shared location, such as an S3 bucket. To do that, you configure a `backend "s3" { ... }` in your Terraform code, and tell it the S3 bucket and folder where that `.tfstate` file should be stored. Once again, if you have Terraform code in multiple folders (`foo` and `bar`), you are going to store separate `.tfstate` files for them in separate folders of your S3 bucket (or separate S3 buckets).

Managing multiple `.tfstate` files is a bit of a pain, so why would you do it? The main reason is to have isolation between different environments or components. For example, you typically want to keep all the infrastructure in your staging environment completely separate and isolated from the infrastructure in your production environment. That includes keeping their Terraform code and `.tfstate` files separate, so if you make an error while editing something in staging, you don't accidentally mess up prod. Charity Majors has a [great blog post about separating environments with Terraform](https://charity.wtf/2016/03/30/terraform-vpc-and-why-you-want-a-tfstate-file-per-env/).

Of course, you may want to separate not only environments, but also different types of infrastructure within an environment. For example, whereas you might deploy a VPC once, and then not touch it again for a year, you may deploy your apps 10 times per day. If both the VPC and those apps are in the same Terraform code (and same `.tfstate` file), then you are putting your VPC at risk of an accidental error 10 times per day, completely unnecessarily. So you may want to have the VPC and app code in separate folders, and therefore, with separate `.tfstate` files.

The problem with this is that you then end up with tons of `backend "s3" { ... }` blocks to manage. Moreover, you have to run `terraform apply` in tons of folders to bring up a single environment.

This is where Terragrunt comes in. It can fill in the config in the `backend "s3" { ... }` block automatically, based on the folder structure of the Terraform code itself. See [here for more info and examples](https://github.com/gruntwork-io/terragrunt#keep-your-remote-state-configuration-dry). It also allows you to use the `xxx-all` commands to deploy a bunch of folders in parallel, with a single command. 

Hopefully, the above explains to you why you either (a) *should* have separate `.tfstate` files per folder or (b) not have separate folders. Moreover, it also hopefully explains how Terragrunt determines the path for the `.tfstate` files in your S3 bucket.
***

**mavogel** commented *Nov 29, 2017*

@josh-padnick thank you for the hint with the `terraform_remote_state`. I got it working with this approach but it only solves @boldandbusted's issue partially.

### My setup
1. Assume I have the following structure
```sh
â”œâ”€â”€ non-prod
â”‚Â Â  â”œâ”€â”€ env.tfvars
â”‚Â Â  â”œâ”€â”€ eu-central-1
â”‚Â Â  â”‚Â Â  â””â”€â”€ dev
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ aws
â”‚Â Â  â”‚Â Â      â”‚Â Â  â””â”€â”€ terraform.tfvars
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ env.tfvars
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ services
â”‚Â Â  â”‚Â Â      â”‚Â Â  â”œâ”€â”€ random-number-service
â”‚Â Â  â”‚Â Â      â”‚Â Â  â”‚Â Â  â””â”€â”€ terraform.tfvars
â”‚Â Â  â”‚Â Â      â”‚Â Â  â””â”€â”€ random-number-service-xx
â”‚Â Â  â”‚Â Â      â”‚Â Â      â””â”€â”€ terraform.tfvars
â”‚Â Â  â”‚Â Â      â””â”€â”€ swarms
â”‚Â Â  â”‚Â Â          â”œâ”€â”€ swarm1
â”‚Â Â  â”‚Â Â          â”‚Â Â  â””â”€â”€ terraform.tfvars
â”‚Â Â  â”‚Â Â          â””â”€â”€ swarm2
â”‚Â Â  â”‚Â Â              â””â”€â”€ terraform.tfvars
â”‚Â Â  â””â”€â”€ terraform.tfvars
â”œâ”€â”€ prod
```

2. I declared the dependencies like `terragrunt` in `swarms/swarm1/terraform.tfvars`
```sh
terragrunt = {
  terraform {
    source = "../../_modules/swarm"
  }
  dependencies {
    paths = ["../../aws"]
  }
  include {
    path = "${find_in_parent_folders()}"
  }
}
```

3. The `main.tf` of my `swarm-module` uses the remote state
```sh
data "terraform_remote_state" "aws" {
  backend = "s3"

  config {
    bucket = "my-bucket"
    key    = "${var.aws_region}/dev/aws/terraform.tfstate"
    region = "${var.aws_region}"
  }
}
```

hence the access `"${data.terraform_remote_state.aws.ami}"` works fine ðŸ˜ƒ 

**But** it's still not possible to run `terraform plan-all` in `non-prod/eu-central-1` or `non-prod/eu-central-1/dev` because for planning the remote state is yet stored in the `s3`bucket.

**Workaround**: I ended up with a script.... Well at least I can deploy all swarms and then all services in parallel.
```sh
cd non-prod/eu-central-1/dev/aws && terragrunt apply-all --terragrunt-non-interactive -auto-approve
cd ../swarms && terragrunt apply-all --terragrunt-non-interactive -auto-approve
cd ../services && terragrunt apply-all --terragrunt-non-interactive -auto-approve
```

### Questions
1. Is this an issue/feature request for `terraform` or `terragrunt` to add this capability?
1. If it is for `terragrunt`, an idea could be to check the output of the depending modules and see if they match with the ones requested in the `"${data.terraform_remote_state.xx}` and give ðŸš¦ green if it's ok. Would that be an option? I'd also be interested in @brikis98 opinion about that ðŸ˜ƒ  (btw awesome book you wrote)

***

**josh-padnick** commented *Dec 1, 2017*

@mavogel Hmm, that's an interesting issue you highlight. If I'm understanding things correctly, this issue only shows up _before_ you've run `terraform apply` / `terragrunt apply` on the dependent module (in your case `../../aws`). Honestly, I think to date we've just done an initial apply and only then used `plan`. Or am I missing something about your use case?
***

**mavogel** commented *Dec 3, 2017*

@josh-padnick the issue show up if I run `terragrunt plan-all` in the `eu-central-1` or even `dev` folder to ramp up a whole environment. `terragrunt` detects the dependencies correctly but it cannot plan because the remote-state is not set for `aws` when it plans `services` hence it fails.
Later when `aws` and the `swarms` are set up, I can easily add new services and run `plan-all` in the `dev` folder because all dependencies for services are setup and the remote state exists.

## Current behaviour
Now assume I want a `qa` env in `eu-central-1`, then I can't run `plan-all` in the `qa` folder. I have to go in the `aws`, then `swarm`, then `services` and run in each `apply-all` to setup everything and to verfy all plans work. **But** on the other hand an `apply-all` works fine because the dependencies are fulfilled and each remote state exist before the next module will be created.

## Desired/expected behaviour
Add an `qa` env and run `plan-all` and then `apply-all` in that folder. Therefore `plan-all` should kind of *mock* the remote state of the dependency and maybe verify the outputs of that module, because they are the ones which I'll read in the module.

```sh
$ cd non-prod/eu-central-1/dev
# FAILS but I want it to work as well
$ terragrunt plan-all --terragrunt-non-interactive -auto-approve 
# on the other hand works cuz the remote states exist
$ terragrunt apply-all --terragrunt-non-interactive -auto-approve
```
 
Hope this explains my case better ðŸ˜ƒ 
***

**brikis98** commented *Dec 3, 2017*

@mavogel Yup, this is a known limitation of `plan-all` with modules that use `terraform_remote_state`. What solution would you propose? Terragrunt can't influence the behavior of the `terraform_remote_state` data source, and even if it could, what data could it even return? Those `.tfstate` files could contain *anything*â€”VPC IDs, hosted zone IDs, URLs, ASG names, etcâ€”and even if you could figure out what data to put in there, any sort of "mocks" would be very likely to create weird errors. 
***

**mavogel** commented *Dec 3, 2017*

@brikis98 True those `terraform_remote_state` could contain everything. Hence mocking doesn't make any sense.
IMHO the cleanest solution is that `terraform` could/should get the feature build the dependency graph over modules and pass input/outputs over their *borders*.

So we stay with the limitation of `plan-all` that in the first ramp up the modules with dependent `remote_states` have to be built and `applied` one by one ðŸ˜ƒ 
***

**brikis98** commented *Dec 3, 2017*

Hehe, yea, that's as far as I was able to get :)

OK, marking this issue as closed. If there are other questions on this topic, feel free to open a new one.
***

