# Improve documentation regarding mono-repo terraform and terragrunt scripts/config

**Zyntogz** commented *Aug 22, 2020*

Hey folks,

i recently discovered terragrunt and what should i say: it has simply gorgeous functionalities. I wished i knew from its existence long ago so all these awkward improvised terraform wrapper script to roll out to different environments/regions would have been avoided. So many thanks for that!

During the read-through of the docs one major question rose: Why not have one repo containing the terragrunt config structures (live repo in your docs) and the terraform scripts (module repo in your docs)? When you have an adequate project structure (terraform and terragrunt folder on the same file system hierarchy level) it seems totally reasonable. Using git clone and specifying a local path as source in the respective terragrunt configs one could even leverage branching possibilities (as far as i know when specifying a git repo as source terragrunt is only able to check out specific tags). Am I missing something here? Are my thoughts legit? Are there advantages/disadvantages i don't see? 

I think other people could ask themselves these questions too. If there would be a doc explaining these specific design choices (for example you did so with monorepo vs polyrepo regarding modules: https://github.com/gruntwork-io/terragrunt-infrastructure-modules-example) newer terragrunt users could get up&running faster with better design decisions.

Many thanks and I appreciate your work!
<br />
***


**yorinasub17** commented *Aug 23, 2020*

You are right that we can definitely improve our docs around terragrunt best practices. Let me take a stab at answering your question here:

> Why not have one repo containing the terragrunt config structures (live repo in your docs) and the terraform scripts (module repo in your docs)?

The main reason we recommend separating the terraform scripts (`infrastructure-modules`) from the terragrunt live config (`infrastructure-live`) is so that you can leverage immutable infrastructure. The easiest way to understand this is the use case around rolling out new infrastructure versions:

Suppose you have made a new improvement to one of your modules (e.g., your `vpc` module). This might have major backwards incompatibilities that you would like to thoroughly test out (e.g., adding a new NACL that closes off a set of ports).

In a mono-repo setup that combines both repos, you would make the change to the module in `master` branch. Then you would proceed to deploy it in `dev` by pointing to the local folder and running `apply`. You would then work through some tests in the `dev` environment to make sure closing that port doesn't cause issues. This may take days depending on how big your app is and how significant the change is.

Now imagine that in the mean time, a critical security vulnerability was found where you need to close off a single port in the VPC ASAP, in all your environments. However, you haven't fully tested the current changes that are in `master` yet, and you don't want to include that in your patch when rolling out to `stage` and `prod`.

You could handle this by cutting releases and branches for hot patches in your mono repo, but that can be confusing (which branch is `dev` on and what branch of the `modules` is that deploying? do I need to run terragrunt from the `v0.10.5` tag for `dev`, or can I just use `master`?).

If you have the `modules` separate from the `live`, you can track these in separate versions in the `modules` repo and terragrunt points to those immutable versions. E.g., you can bump `dev` to `v0.11.0`, while `stage` and `prod` is on `v0.10.5`, and when there is a hot patch, you roll out `v0.10.6` and `v0.11.1` of the modules to `stage`/`prod` and `dev`. Then your live config becomes fairly simple in terms of branching: you always deploy from `master`, with each environment pointing to different versions of your infrastructure.

Another reason is how testing cycles differ across the two, and how releases are fundamentally different, which Gruntwork covers in more depth in their [How to configure a production grade CI/CD setup for apps and infrastructure code](https://gruntwork.io/guides/automations/how-to-configure-a-production-grade-ci-cd-setup-for-apps-and-infrastructure-code/#types_of_infrastructure_code).
***

**Zyntogz** commented *Aug 23, 2020*

Many thanks for your time invested, i really appreciate it and i hope others can use the information generated here, too. Or maybe it could be processed in some kind of FAQ or Design Choices chapter or such. After reading your posted link and a blog entry (https://blog.gruntwork.io/how-to-use-terraform-as-a-team-251bc1104973) I think i got your point there. 

In the "old world" where infrastructure was provisioned to you by the ops team you basically used for example 4 branches for environment orchestration (one for each staging environment), worked on local branches and then, when you were optimistic that you got a stable build, commited/merged to dev and CI/CD deployed the stuff on your infrastructure. How the infrastructure got there was quite irrelevant for you as a developer. After time, you saw everything was fine and you merged your changes from stage to stage. Each stage had its own CI/CD pipeline which deployed your recently pushed stuff to the respective infrastructure until it was successfully running in prod.

However, in the "new world" developers are also DevOps guys and have to provision the infrastructure for their stuff by themselves. In my previous projects without terragrunt, we had these naughty wrapper scripts for orchestration using environment variables to deploy to different stages/regions. The terraform codebase itself was relatively DRY because of this (however there still were huge mapping dictionaries for ressource provisioning for different environments and such stuff). Furthermore there was still copy&paste in shared ressources of the branches (for example the wrapper script). But still, for specification of what environment shall use which code version, we had to use different branches. 

Because terragrunt centrally abstracts all the environment stuff away from your modules, you don't need to have different branches for your terraform module repo for the different stages anymore. And because terragrunt shall be always the abstraction of the real-world in the cloud and hence, always up to date with each environment, you don't need different branches for different stages in your live repo, too. At least as I understood now. So maximum DRYness :) 

However, there are still question marks mainly regarding CI/CD (despite reading this little piece of gold: https://gruntwork.io/guides/automations/how-to-configure-a-production-grade-ci-cd-setup-for-apps-and-infrastructure-code/#cicd_workflows):

- When you push something into the live repo, how to make your CI/CD pipeline run no unnecessary tests? For example, if you changed something in your dev environment, why redeploy everything? Or is it always "just deploy everything, it will make unnecessary stuff but that doesn't harm anyone"?

- Regarding your terraform module repo, whenever you push something into the master there, i think the CI/CD stuff shall go off and test/deploy your stuff to the dev environment right? But to do so, it would have to check out your live repo and modify respective terragrunt config sources of changed modules right? If that's true and if i'm not missing something, that seems like a lot of effort. Furthermore, when deploying new modules I understood that you have to push your module first, let the CI/CD do unnecessary stuff and afterwards push your live config so the module gets actually deployed, right?

- In my current project we go completely serverless using lambda. Hence, i planned to put terraform modules and source code in a file system structure that allows relative references between them. However, because terraform module repo has only a master branch and our application repo is planned to have a dev and master branch we shall use two repos. I am planning to set up the CI/CD so that every tagged commit/push into master triggers, like in question 2, a dev deployment.
Now here comes the question: During a deployment, what could be a "good practice" to access the application code repo while having terraform code in a different repo? Clone it via terragrunt with a before_hook or something like that? Or shall, like i did it before, the terraform code and the application code be in the same repo? 

Many thanks and have a nice day 
***

**yorinasub17** commented *Aug 27, 2020*

I think it's important to clarify that everything is about trade offs, and we've recommended a set of options based on our opinions about what trade offs to optimize for.

In our case, we were optimizing for stability and understandability over absolute speed of delivery. Immutable tags allow you to keep a lot of things stable, while having separate repos with different version tags allows you to track what version of the infra code is deployed at any given point in time.

Keeping that in mind:

> When you push something into the live repo, how to make your CI/CD pipeline run no unnecessary tests? For example, if you changed something in your dev environment, why redeploy everything? Or is it always "just deploy everything, it will make unnecessary stuff but that doesn't harm anyone"?

The canonical way to handle this is to implement scripts that can detect which modules changed, and only invoking the pipeline for those modules. You can leverage some combination of `git diff`, `grep`, and `find` to identify terragrunt modules and act accordingly.

> Regarding your terraform module repo, whenever you push something into the master there, i think the CI/CD stuff shall go off and test/deploy your stuff to the dev environment right? But to do so, it would have to check out your live repo and modify respective terragrunt config sources of changed modules right? If that's true and if i'm not missing something, that seems like a lot of effort. Furthermore, when deploying new modules I understood that you have to push your module first, let the CI/CD do unnecessary stuff and afterwards push your live config so the module gets actually deployed, right?

You wouldn't want to do this everytime you merge to `master`, but rather every time you cut a release. The CI pipeline is different for `modules` and `live` config. In particular, getting into the habit of "unit testing" your modules to ensure they work can catch a lot of bugs before you go in and updating a live environment.

Internally at Gruntwork, our CI pipeline for `master` on the modules repo runs a suite of [terratest tests](terratest.gruntwork.io) to make sure the modules are working as expected in isolation. Then, when we feel that the modules are in a state that can be deployed to an actual environment, we cut a release tag to indicate that it can be deployed to an existing environment.

At this point, you can decide what to do. For example, you can have a script that automatically bumps the tag and deploys it for `dev`, and then have [renovatebot](https://renovate.whitesourcesoftware.com/) setup to open PRs to autobump each environment to that module version each time you cut a release. The advantage of this is that you can choose to merge that `stage` PR after confirming it works for `dev`. Or you can decide you want more control over releases (e.g., avoiding an infrastructure failure while your app team is running tests on the dev env) and opt for PRs for all the environments.

> Now here comes the question: During a deployment, what could be a "good practice" to access the application code repo while having terraform code in a different repo? Clone it via terragrunt with a before_hook or something like that? Or shall, like i did it before, the terraform code and the application code be in the same repo?

I would recommend a process in the application repo to push immutable versioned packages of your serverless application to AWS, and reference those tags in the terraform code. This could be as simple as building the zip file and pushing to s3, to as complex as creating a [SAR](https://aws.amazon.com/serverless/serverlessrepo/). This is similar to what you would do for Docker or Packer apps, where the app repo builds a container image / AMI and publishes that.

In this model, terraform/terragrunt doesn't need to touch the application repo during the deploy process, and instead have everything handled in the cloud.

Just to complete the picture, to do CD to a dev environment here, you would have the build process for the app repo clone the infra live repo and commit a change to the tag in the live config that manages the serverless app.
***

**Zyntogz** commented *Sep 2, 2020*

Many thanks for your reply. I think you was a great help! The many parts start assembling into a whole picture and everything is becoming much clearer to me.  I definitely will apply these techniques while using terragrunt :)
***

