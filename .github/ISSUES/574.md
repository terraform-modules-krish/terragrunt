# Workaround for string interpolation in tfvars files?

**oerp-odoo** commented *Sep 21, 2018*

As I understand string interpolation does not work on `tfvars`, but terraform recommends using string interpolation when writing terraform configuration. And I find it useful, because I don't want to write specific ids/values in terraform confi files, when they can be changing. I want to reference resource's variables.

So what would be the solution when using terragrunt and `tfvars` as specified in examples?

_Issue at hand is at `security_group_ec2/terraform.tfvars` file where I left a comment._

Currently I have this structure for modules repo (now only one module, but it the one I got stuck because of string interpolation limitations):

```
terraform-modules/
└── security_group_ec2
    ├── main.tf
    ├── outputs.tf
    └── variables.tf
```

Inside that module:

**main.tf:**

```
resource "aws_security_group" "sg" {
  name = "${var.name}"
  description = "${var.description}"
  vpc_id = "${var.vpc_id}"

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["${var.ext_blocks}"]
  }

  egress {
    from_port       = 0
    to_port         = 0
    protocol        = "-1"
    cidr_blocks     = ["0.0.0.0/0"]
  }
}

terraform {
  # The configuration for this backend will be filled in by Terragrunt
  backend "s3" {}
}
```
**outputs.tf:**
```
output "sg_id" {
  value = "${aws_security_group.sg.id}"
}
```
**variables.tf:**
```
variable "name" {
  description = "Name of security group"
}
variable "description" {
  description = "Description of security group"
}
variable "vpc_id" {
  description = "Virtual Private Cloud ID to assign"
}
variable "ext_blocks" {
  description = "External IPs to allow connecting through Port 22"
  default = "list"
}
```

Then live part:
```
stage
├── data-storage
├── services
│   └── security_group_ec2
│       └── terraform.tfvars
├── terraform.tfvars
└── vpc
    └── main.tf
```

**security_group_ec2/terraform.tfvars:**

```
terragrunt = {
  terraform {
    source = "git::git@github.com:oerp-odoo/terraform-modules.git//security_group_ec2?ref=v0.1.0"
  }
}

name = "stage-ec2"
description = "Security Group for EC2 stage environment"
vpc_id = "${var.aws_default_vpc.default.id}"  # How to workaround it? Cause I can't use this syntax here.
ext_blocks = ["1.2.3.4/32"]
```
**vpc/main.tf:**

```
resource "aws_default_vpc" "default" {
    tags {
        Name = "Default VPC"
    }
}
```
**terraform.tfvars:**
```
terragrunt = {
  remote_state {
    backend = "s3"
    config {
      bucket         = "some-bucket-name"
      key            = "stage/terraform.tfstate"
      region         = "eu-central-1"
      encrypt        = true
      dynamodb_table = "my-lock-table"

      s3_bucket_tags {
        owner = "terragrunt live stage"
        name  = "Terraform state storage for live stage"
      }

      dynamodb_table_tags {
        owner = "terragrunt live stage"
        name  = "Terraform lock table for live stage"
      }
    }
  }
}
```

Maybe there are some best practices that I'm missing and I should structure this a bit differently when it involves resources I need to reference dynamically?
<br />
***


**brikis98** commented *Sep 22, 2018*

Do all of your interpolation in the `.tf` files in your `terraform-modules`, not in the `.tfvars` files. Example:

1. Put `vpc/main.tf` into your `terraform-modules` repo. 
1. Create `stage/vpc/terraform.tfvars` to deploy the vpc module. Make sure this module has the VPC ID as an output and stores its state in a backend such as S3.
1. Update `terraform-modules/security_group_ec2` to use the [terraform_remote_state](https://www.terraform.io/docs/providers/terraform/d/remote_state.html) data source to read the state stored by the VPC module and to set the `vpc_id` parameter of `aws_security_group.sg` to something like `data.terraform_remote_state.vpc.vpc_id`.
***

**oerp-odoo** commented *Sep 22, 2018*

Thanks for the help.
 I updated just how you said (I hope), but I don't understand why after running `terragrunt apply-all` (running it from `stage` directory) I get prompt to enter remote config info? Like the name of S3 bucket, key of S3 bucket? Shouldn't all that be read by configuration from `timefordev-ias/stage/terraform.tfvars`? And it asks me to enter it twice (I guess for two modules).
Is this intended or I missed something? I mean I need to specify that per module? Should I just copy/paste remote config data in all of those prompts (as in `timefordev-ias/stage/terraform.tfvars`)?

What I changed:
I removed vpc resource from live directory and instead created module in module repo (vpc/main.tf, vpc.variables.tf, vpc.outputs.tf).

I added this in `security_group_ec2` module (and then referenced that data source output `vpc_id`, which is an output of `vpc` module)

```
data "terraform_remote_state" "vpc" {
  backend = "s3"
}
```
Then I updated my modules repo to have tag `v0.2.0` and updated code to use that code in live dir.

Then: 
`cd timefordev-ias/stage`
`terragrunt apply-all`

Then after initialization, I got this prompt:

```
Initializing the backend...
bucket
  The name of the S3 bucket

Initializing the backend...
bucket
  The name of the S3 bucket

  Enter a value:   Enter a value: 
```

My current modules structure: https://github.com/oerp-odoo/terraform-modules
***

**oerp-odoo** commented *Sep 22, 2018*

OK, so I managed to solve problem with asked input by including this (into each .tfvars file that calls module):

```
  include {
    path = "${find_in_parent_folders()}"
  }
```
Though after initializing, it does fail to apply variables.
```
Terraform has been successfully initialized!

You may now begin working with Terraform. Try running "terraform plan" to see
any changes that are required for your infrastructure. All Terraform commands
should now work.

If you ever set or change modules or backend configuration for Terraform,
rerun this command to reinitialize your working directory. If you forget, other
commands will detect it and remind you to do so if necessary.
[terragrunt] [/home/oerp/src/timefordev-ias/stage/vpc] 2018/09/22 20:11:47 Running command: terraform apply -input=false -auto-approve

Error: Required variable not set: vpc_id



Error: variable ext_blocks should be type string, got list


[terragrunt] [/home/oerp/src/timefordev-ias/stage/services/security_group_ec2] 2018/09/22 20:11:49 Module /home/oerp/src/timefordev-ias/stage/services/security_group_ec2 has finished with an error: Hit multiple errors:
exit status 1

Error: provider.aws: "region": required field is not set


[terragrunt] [/home/oerp/src/timefordev-ias/stage/vpc] 2018/09/22 20:11:51 Module /home/oerp/src/timefordev-ias/stage/vpc has finished with an error: Hit multiple errors:
exit status 1
[terragrunt] 2018/09/22 20:11:51 Encountered the following errors:
Hit multiple errors:
exit status 1
Hit multiple errors:
exit status 1
```

It does not look like its is applying those variables at all. I changed `vpc_id` to use data source to apply value instead of manually specifying it when calling module, so I don't get it why it still tries to apply `vpc_id`.

Also `ext_blocks` is specified as `type="list"`, so I also don't understand why terraform/terragrunt gives me error that it expects string?

And regarding aws and region, I explicilty add `provider aws` on each module just as in examples, but it does look like it is ignoring those?
***

**oerp-odoo** commented *Sep 22, 2018*

OK, I managed to initialize `vpc` module for `stage`, but I'm still struggling with `security_group_ec2` initialization, specifically `vpc_id`. It just outputs that there is no such attribute `vpc_id`. And I had to hardcode `bucket` and `key` in config of remote state. For some reason, it throws `unknown variable referenced` (even if I have defined it as required variables in module..).

So here how it looks like now:

```
data "terraform_remote_state" "vpc" {
  backend = "s3"
  config {
    # bucket = "${var.bucket}"
    bucket = "dod-terraform-state-storage"
    # key = "${var.key}"
    key = "stage/terraform.tfstate"
    region = "${var.aws_region}"
  }

}
```

It gives this output:

```
Error: Error running plan: 1 error(s) occurred:

* aws_security_group.sg: 1 error(s) occurred:

* aws_security_group.sg: Resource 'data.terraform_remote_state.vpc' does not have attribute 'vpc_id' for variable 'data.terraform_remote_state.vpc.vpc_id'
```
Though I do output this variable (defined in vpc module outputs):
```
output "vpc_id" {
  value = "${aws_vpc.vpc.id}"
}
```
Though it looks like other people have this same problem. It looks like it is not working at all..
https://github.com/hashicorp/terraform/issues/12316

***

**brikis98** commented *Sep 23, 2018*

Did you successfully run `terragrunt apply` in the VPC module? Also, this config looks wrong:

```hcl
data "terraform_remote_state" "vpc" {
  backend = "s3"
  config {
    # bucket = "${var.bucket}"
    bucket = "dod-terraform-state-storage"
    # key = "${var.key}"
    key = "stage/terraform.tfstate"
    region = "${var.aws_region}"
  }
}
```

The `key` should be the path where the VPC module is storing its state. That's probably `stage/vpc/terraform.tfstate`. Terraform's error handling here is, unfortunately, quite poor.
***

**oerp-odoo** commented *Sep 23, 2018*

Yes I was able to do `apply` in VPC module.

And you were right, key path was incorrect. I looked it up via AWS interface and it was created as `services/vpc/terraform.tfstate` (I moved vpc to `services`, because I accidentally created outside services). After correcting it, I was able to create security group and even by passing `bucket` and `key` as variables (don't know why it was throwing `unknown variable reference` error before).

Now I think it looks fine.

One last question (kind of two), is there a way to run `terragrunt apply-all` by specifying all modules locally?

If I do that per module, i can specify it with `terragrunt apply-all --terragrunt-source relative-path-to-module`, but this seems to not work when I need to specify more than one module? I tried doing the same, but instead of specifying module path, I specified all those modules parent directory. Though I got error that there is no such file.

And second one (kind of related with first one), I was only able to run apply-all from `stage` directory by using remotely pushed code (those same modules), but it looks like terragrunt keeps its downloaded local copy and does not update it if I force push the update? I mean I had tag `v0.2.0`, then commited something new, but did not want to add new commit, so I ammended previous commit and then forced pushed it. But terragrunt was ignoring that and was using old code which actually did not exist on remote anymore. It only picked up new code after I added new tag and pushed new code (without ammending).
***

**brikis98** commented *Sep 23, 2018*

> One last question (kind of two), is there a way to run terragrunt apply-all by specifying all modules locally?

https://github.com/gruntwork-io/terragrunt#testing-multiple-modules-locally

> I mean I had tag v0.2.0, then commited something new, but did not want to add new commit, so I ammended previous commit and then forced pushed it. 

That's a pretty big anti-pattern. Tags should be immutable pointers to a commit. Don't modify them! Instead, push a new tag. This has nothing to do with Terragrunt, but just a basic best-practice with tags.

Terragrunt follows this best practice and assumes tags are immutable, so there's no need to re-download. If you want to force it to re-download, set the `--terragrunt-source-update` flag.
***

**oerp-odoo** commented *Sep 24, 2018*

Yes, I usually do not do that, but while I was testing I did not want to
create new tag for every small change. I guess I could point to Head
instead of tag when testing.

Also is there a way to run apply-all for all modules at once by specifying
local path, just as you can per module?

On Sun, Sep 23, 2018, 23:10 Yevgeniy Brikman <notifications@github.com>
wrote:

> One last question (kind of two), is there a way to run terragrunt
> apply-all by specifying all modules locally?
>
> https://github.com/gruntwork-io/terragrunt#testing-multiple-modules-locally
>
> I mean I had tag v0.2.0, then commited something new, but did not want to
> add new commit, so I ammended previous commit and then forced pushed it.
>
> That's a pretty big anti-pattern. Tags should be immutable pointers to a
> commit. Don't modify them! Instead, push a new tag. This has nothing to do
> with Terragrunt, but just a basic best-practice with tags.
>
> Terragrunt follows this best practice and assumes tags are immutable, so
> there's no need to re-download. If you want to force it to re-download, set
> the --terragrunt-source-update flag.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/gruntwork-io/terragrunt/issues/574#issuecomment-423844316>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AHc3eo0V2WccinJFkqqaWqtUtC4F_cVAks5ud-qzgaJpZM4W0SZv>
> .
>

***

**brikis98** commented *Sep 24, 2018*

> Also is there a way to run apply-all for all modules at once by specifying
local path, just as you can per module?

Yes, I pasted this link to answer that earlier: https://github.com/gruntwork-io/terragrunt#testing-multiple-modules-locally

***

**oerp-odoo** commented *Sep 25, 2018*

Oh, don't know how I missed it..:). It seems it works only if I specify absolute path. I tried relative path before (like `../../../terraform-modules`), but I would get no such file or directory exists. But if specifying absolute path is working, then its OK.

Though I noticed in your documentation, you are giving this example: `terragrunt apply-all --terragrunt-source: /source/infrastructure-modules`. Notice colon after `--terragrunt-srouce`. I think thats incorrect, because I get error if I do that.

Anyway thanks for helping out.
***

**brikis98** commented *Sep 25, 2018*

> Though I noticed in your documentation, you are giving this example: terragrunt apply-all --terragrunt-source: /source/infrastructure-modules. Notice colon after --terragrunt-srouce. I think thats incorrect, because I get error if I do that.

Whoops, fixed now, thx.
***

